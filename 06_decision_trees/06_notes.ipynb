{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c52476",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are powerful, versitile MLA's that can perform both classification & regression tasks, and event multioutput tasks. They can be used to fit complex datasets.\n",
    "\n",
    "### Advantages: \n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "### Disadvantages: \n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f410e",
   "metadata": {},
   "source": [
    "## Training & Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6cf95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # Petal length & width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b27ae",
   "metadata": {},
   "source": [
    "You can visualize the trained DTree by first using the `export_graphviz` to output a graph definition file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45c5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file='iris_tree.dot',\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7cb55",
   "metadata": {},
   "source": [
    "![Decision Tree](iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4226d",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Suppose you had an iris flower and you wanted to classifiy it - you would start at the root node, which asks if the petal lenght is greather than 2.45cm.\n",
    "\n",
    "If it is, than you would move to the left child node (which is called a `leaf node` since it doesn't have any children nodes), so it does not ask any questions - it outputs a predicted class of `setosa`.\n",
    "\n",
    "If the petal length is less than 2.45cm, than you would move down to right child node which asks another question - is the petal width smaller than 1.75cm?\n",
    "\n",
    "If it is, you would move to the left leaf node which classifies the flower as `versicolor`. If it isn't, than you would move to the right leaf node which classifies the flower as a `virginica`.\n",
    "\n",
    "The `sample` attributes counts how many training instances that node applies to,\n",
    "The `value` attributes tells you how many training instances of each class this node applies to,\n",
    "The `gini` attribute tells you how \"pure\" the node is by if all the training instances it applies to belong the same class,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c1c22",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "\n",
    "A decision tree can also estimate eht probability that an instance belogs to a particular class. First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances `k` in this node. \n",
    "\n",
    "For example, suppose you found a flower whose petals are tcm long and 1.5cm wide. The corresponding leaf node is the depth-2 left node, so the Dtree should output the following probabilities:\n",
    "\n",
    "- 0% for Iris Setosa (0/54)\n",
    "- 90.7% for Iris Versicolor (49/54)\n",
    "- 9.3% fpr Iris Virginicia (5/54)\n",
    "\n",
    "And if you ask it to predict the class, it should output `Iris Versicolor` because it has the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72a7990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d52345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e3d69",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "Scikit-Learn uses the `Classification & Regression Tree (CART)` algorithm to train DTrees (also called growing trees).\n",
    "\n",
    "The algorithm works by first splitting the training set into two subsets using a single feature `k` and a theshold `t` (e.g petal length > 2.45cm). It chooses `t` and `k` via searching for the pair (`k, t`) that produces the purest subsets weighted by their size via the CART cost function.\n",
    "\n",
    "Once the CART algorithm has successfully split the training set in two, it splits the subsets recursivly until the max_depth hyperparameter is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88997c85",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict thee Dtree's freedom during training, which is called regularization. The regularization hyperparameters depend on the algorithm used, but generally, regularization can be controlled through the `max_depth` hyperparameter (the default value is `None`, which means unlimited depth). Reducing the `max_depth` will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The `DecisionTreeClassifier` class has a few other paremters that similarly restrict the shape of DTree:\n",
    "\n",
    "   - `min_samples_split`: The minimum # of samples a node must have before it can be split\n",
    "   - `min_samples_leaf`: The minimum # of samples a leaf node must have.\n",
    "   - `min_weight_fraction_leaf`: Same as `min_samples_leaf` but as expressed as a fraction rather than a #.\n",
    "   - `max_leaf_nodes`: The maximum # of leaf nodes.\n",
    "   - `max_features`: The maximum # of features that are evaulated for splitting each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d8787",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Decision Treee's are also capable of performing regression tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c257d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dffe961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_reg,\n",
    "    out_file='iris_tree_reg.dot',\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2417b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng iris_tree_reg.dot -o iris_tree_reg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9593d3",
   "metadata": {},
   "source": [
    "![Decision Tree](iris_tree_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5fd0f",
   "metadata": {},
   "source": [
    "The prediction in each leaf node is the average target value of the samples in that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f59c25",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e7424",
   "metadata": {},
   "source": [
    "The largest issue with Dtree's is that they are very sensative to small variations in the training data. For example, if you remove the widest `Iris Versicolor` from the iris traning set & train a new Dtree, you may get a model that looks very different from a model that has that data point included.\n",
    "\n",
    "Random Forests can limit this instability averaging predictions over many trees as we will see in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841f506",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
