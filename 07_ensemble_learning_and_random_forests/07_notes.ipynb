{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7b1d9a",
   "metadata": {},
   "source": [
    "# Ensemble Learning & Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57564a",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a KNN classifier, and perhaps a few more.\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict hte class that gets the most votes. This majority-vote classifier is called a **hard voting** classifier.\n",
    "\n",
    "Somewhat suprisingly, this voting classifier often acheives a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a `weak learner` (e.g it only does slightly better than random guesing), the ensemble can still be a `strong` learner (e.g achieves a high accuracy rating), provided there are a sufficient # of weak learners and they are sufficiently diverse due to the `law of large numbers`.\n",
    "\n",
    "The `law of large numbers` states that if you perform the same experiment a large number of times, the average of the results obtained from the experiemnts should be close to the expected value and tends to become closer to the expected value as more trials are performed. For example, if you flip a coin a 1000 times with a probability of 51% of getting heads & 49% of getting tails, you would expec that with each flip you would get closer 510 heads and 490 tails.\n",
    "\n",
    "The following code creates and trains a voting classifier in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9592f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting=\"hard\")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6071f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9\n",
      "RandomForestClassifier 0.85\n",
      "SVC 0.95\n",
      "VotingClassifier 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29516f10",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate the class probabilities (e.g they all have a `predict_proba` method), then you can tell Scikit-Learn to perdict the class with the highest class probability, averaged over all the individual classifers, which is called `Soft Voting`. `Soft Voting` often achieves higher performance relative to hard voting because it gives more weight to highly confident votes. \n",
    "\n",
    "All you need to do is set `voting=\"hard` to `voting=\"soft\"`, and ensure that all the classifiers can estimate class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84945c",
   "metadata": {},
   "source": [
    "## Bagging & Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithems (e.g SVC, KNN, ect), and another is to use the same training algo for every predictor and then train them on different subsets of the training set.\n",
    "\n",
    "When sampling is performed w/ replacement, this is called `bagging`. When sampling is performed w/out replacement, this is called `pasting`. Only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Once all the predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically called the `statiscial mode` (e.g the most frequent prediction made, just like a hard voting classifier) for classification, and average for regression.\n",
    "\n",
    "Each individual predictor hass a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de6fa1",
   "metadata": {},
   "source": [
    "### Bagging & Pasting in Scikit-Learn\n",
    "\n",
    "`Scikit-Learn` has a very simple API for both bagging and pasting w/ the `BaggingClassifier` class (or `BaggingRegressor` for regression).\n",
    "\n",
    "* `n_estimators` = Ensemble of n classifiers (e.g 500 Dtrees:\n",
    "* `max_samples` = Train each estimator on n samples of the trianing data\n",
    "* `bootstrap` = Bagging = True, Pasting = False\n",
    "\n",
    "Overall, bagging oftern results in better models which explains why it is oftern preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca91fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=75, bootstrap=True, n_jobs=1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec432399",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances my be sampled several times for any given predictor, while others may not be sampled at all. By default, a `BaggingClassifier` samples `m` training instances w/ replacement (e.g `bootstrap=True`) where `m` is the size of the training. This means that only ~63% of training instance are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called `out-of-bag` instances. \n",
    "\n",
    "Since a predictor never sees the `out-of-bag` instances during training, it can be evaluated on these instances without the need for a seperate training set. You can evaluate the the ensemble iteslef by averageing out the `oob` evaluations of each predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1ad73ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score: 0.9375\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Validation Score: {bag_clf.oob_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a1e26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809dcd5",
   "metadata": {},
   "source": [
    "## Random Patches & Random Subspaces\n",
    "\n",
    "The `BaggingClassifier` class supports sampling the features as well, which is controlled by two hyperparameters: `max_features` & `bootstrap_features`. This allows each predictor to be trained on a random subset of the input features, and this technique is particulary useful when you are dealing with high-dimensional inputs such as images. \n",
    "\n",
    "Sampling both training instances and features is called the `Random Patches` method, while keeping all training instances but sampling features is called the `Random Subspaces` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3088944",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A Random Forest is an ensemble of Decision Trees, generally trained via the bagging method, typically w/ `max_samples` set to the size of the training set. Instead of using the `BaggingClassifier` and passing it a `DecisionTreeClassifier`, you can instead use the `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fa60fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7ec50",
   "metadata": {},
   "source": [
    "RandomForests introduce extra randomness when growing trees - instead of searching for the very best feature when splitting a node, it searches for the best feautre among a random subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c3241",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781edd0",
   "metadata": {},
   "source": [
    "A great quality of Random Forests is that they make it easy to measure the the relative importance of each feature. \n",
    "\n",
    "Scikit-Learn measures a features importance by looking at how much the tree nodes use that feature to reduce impurity on average (across all trees in the forest). Scikit-Learn computes this score automatically for each feature after training, and then scales the results so that the sum of all importances is equal to 1. These scores can be accessed via the `feature_importances_` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e9bc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08682404757777516\n",
      "sepal width (cm) 0.021006785266283006\n",
      "petal length (cm) 0.44391103027366163\n",
      "petal width (cm) 0.44825813688228033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=1)\n",
    "\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "rnd_clf.fit(X, y)\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bec788",
   "metadata": {},
   "source": [
    "Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31bf66",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct it's predecessor. \n",
    "\n",
    "There are many boosting methods available, but by far the most popualr are `Adaptive Boosting e.g AdaBoost` and `Gradient Boosting`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69fe4b",
   "metadata": {},
   "source": [
    "### AdaBoost          \n",
    "\n",
    "One way for a new predictor to correct it's predecessor is to pay a bit more attention to the training instances that the predecessor underfitterd. This results in new predictors focusing more & more on the hard cases.\n",
    "\n",
    "For example, when training an AdaBoost classifier: \n",
    "\n",
    "1. The algo first trains a base classifier (like a Dtree) and uses it to make predictions on the training set. \n",
    "2. The algo than increases the relative weight of misclassfied training instances.\n",
    "3. Then it trains as second classifier, using the updated weights, and again makes predictions on the training set.\n",
    "4. Repeat steps 2 & 3 until all estimators are finished. \n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictors very much like bagging or pasting except the predictors have different weights depending on their overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65421968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f2bd3",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "\n",
    "Just like `AdaBoost`, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting it's predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new preditor to the residual errors made by the previous predictor.\n",
    "\n",
    "Let's go through an example on how this works, using DTree's as the base predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c867c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "\n",
    "# Train a second DecisionTreeRegressor on the residual errors of the first predictor:\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train, y2)\n",
    "\n",
    "# Train a third DecisionTreeRegressor on the residual errors of the second predictor:\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a15688",
   "metadata": {},
   "source": [
    "Now we have an ensemble of 3 trees - it can make predictions on a new instance by simply adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f1e578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97fbef",
   "metadata": {},
   "source": [
    "A simplier way to train a Gradient Boosting Regression tree is to use Scikit-Learn's `GradientBoostingRegressor` class. Much like the `RandomForestRegressor` class, it has hyperparameters to control the growth of the Decision Trees (e.g `max_depth`, `min_samples_leaf`) as well as hyperparameters to control the ensemble training, such as the number of trees (`n_estimators`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c579dcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.6)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbrt.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2561e8",
   "metadata": {},
   "source": [
    "The `leaerning_rate` hyperparameter scales the contribution of each tree. If you set it to low values, such as 0.1, you will need more trees in the ensemble to fir the training set but the predictions will usually generalize better. This is a regularization technique called `shrinkage`. \n",
    "\n",
    "With a low learning_rate, if you have too few trees you won't fit the training set, but if you have too many trees you will overfit the training set. \n",
    "\n",
    "In order to find the optimal # of trees, you can use early stopping (e.g stop training before you overfit the model). Early stopping can be implemented via the following code by measuring the validaiton error at teach stage of training to find the optimal # of trees, and then finally training another ensemble using the optimal number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66b19c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=299)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=2, n_estimators=299)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=299)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Get the features and target variable\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=300)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "best_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62de06af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_estimators"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2c1026e",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). This can be accomplished by setting `warm_start=True`. The following code stops training when the validaiton error does not improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "178670af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 300):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb53cb",
   "metadata": {},
   "source": [
    "The `GradientBoostingRegressor` class also supports a subsample hyperparameter, whcih specifies the fraction of training instances to be used training each tree. For example, if `subsample=0.25`, then each tree is trained on 25% of the training instances selected randomly. This techniques trades higher bias for lower variance and speeds up training considerably. This is called `Stochastic Gradient Boosting`.\n",
    "\n",
    "It is worth nothing that an optimized version implementation of Gradient Boosting is available in the popular Python Library XGBoost. XGBoost is often an important componenet of the winning entries in ML competitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e93e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5cd6840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22144596276767312"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "val_error = mean_squared_error(y_val, y_pred)\n",
    "val_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96159f09",
   "metadata": {},
   "source": [
    "xgboost has several nice features, such as automaticallly taking care of early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dcd0a0",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e01b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
