{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf4dead",
   "metadata": {},
   "source": [
    "# Introduction to ANN's with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ea362",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron & Backpropogation\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a deep nueral network (dnn). The field of deep learning studies DNN's, and more generally models containing deep stacks of computations.\n",
    "\n",
    "ANN's and DNN's are trained via the `backpropogation` training algorithm, which in short is Gradient Descent using an efficient technique for computing the gradients automatically. The goal of backpropagation is to minimize the error between the predicted output of the network and the actual target values.\n",
    "\n",
    "The Backpropagation algorithm works by handling one mini-batch at a time (e.g, containing 32 training examples each) and goe through the full training set multiple times. Each pass is called an `epoch`. \n",
    "\n",
    "For each training instance the backpropogation algorith works by first making a prediction (forward pass) and measuring the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass) and finally tweaks the connection weights to reduce the error (gradient descent step).\n",
    "\n",
    "* **Initialization**: Initialize the weights and biases of the neural network with small random values. Define the learning rate, which determines the size of the steps taken during the optimization process.\n",
    "* **Forward Pass**: Each mini-batch is fed forward through the network layer by layer. Neurons in each layer perform a weighted sum of their inputs, add a bias, and then apply an activation function to produce the output.\n",
    "* **Calculate Error**: Compare the network's output with the actual target values to calculate the error. The error is typically measured using a loss or cost function, which quantifies the difference between the predicted and actual values.\n",
    "* **Backward Pass**: The goal is to then update the weights and biases in the network to reduce the error by calculateing the gradient of the error with respect to the weights and biases using the chain rule of calculus. This is done via propagating the gradient backward through the network until the algorithm reaches the input layer. to find how much each weight and bias contributed to the error.\n",
    "* **Update Weights and Biases**: Adjust the weights and biases in the direction that reduces the error. This is done by subtracting a fraction of the gradient multiplied by the learning rate. The learning rate determines the step size in the weight and bias updates. It's crucial to choose an appropriate learning rate to balance convergence speed and stability.\n",
    "* **Repeat**: Steps 2 to 5 are repeated for multiple epochs (passes through the entire training dataset) until the network's performance converges to an acceptable level. The backpropagation algorithm essentially iteratively adjusts the weights and biases of the neural network to minimize the error between predicted and actual outputs. This process is an optimization task, and the choice of the loss function, activation functions, and network architecture all play crucial roles in the success of the training process.\n",
    "\n",
    "In order for the algorithm to work properly, the authors made a key change by replacing the step function with the logistic (sigmoid) function. This was essential, because the step function contains only flat segment so there was no gradient to work with (GD doesn't work on a flat surface) while the logistic function has a well defined nonzero derivitiave everywhere, allowing GD to make some progress at every step. The logistic function is an example of an `activation function`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32efb6",
   "metadata": {},
   "source": [
    "### Activation Functions \n",
    "\n",
    "An activation function is a mathematical operation applied to the output of each neuron (or node) in a neural network layer. \n",
    "\n",
    "Activation functions introduces non-linearity to a DNN, allowing it to learn from and model complex patterns in data. Without non-linear activation functions, the entire neural network would behave like a linear model, regardless of its depth.\n",
    "\n",
    "The purpose of the activation function can be summarized as follows:\n",
    "\n",
    "* **Introducing Non-linearity**: Linear transformations (such as weighted sums and biases) are limited to representing linear relationships. By applying non-linear activation functions, the network can learn and approximate non-linear mappings between inputs and outputs.\n",
    "* **Enabling Complex Representations**: The stacking of non-linear activation functions in deep networks enables the modeling of intricate relationships and hierarchies in data, allowing the network to learn and represent complex patterns.\n",
    "\n",
    "Here are some commonly used activation functions in deep neural networks:\n",
    "\n",
    "* **Sigmoid Function (Logistic):**\n",
    "    * _MOST COMMON_\n",
    "    * Outputs values between 0 and 1.\n",
    "    * Historically used in the output layer for binary classification problems, but not as common in hidden layers due to the vanishing gradient problem.\n",
    "* **Hyperbolic Tangent (tanh)**:\n",
    "    * _MOST COMMON_\n",
    "    * Outputs values between -1 and 1.\n",
    "    * Similar to the sigmoid but with a higher output range.\n",
    "* **Rectified Linear Unit (ReLU)**:\n",
    "    * _MOST COMMON_\n",
    "    * Outputs zero for negative inputs and passes positive inputs as is.\n",
    "    * Widely used in hidden layers due to its simplicity and effectiveness in training deep networks.\n",
    "* **Leaky ReLU**:\n",
    "    * Similar to ReLU but allows a small, non-zero gradient for negative inputs, addressing the \"dying ReLU\" problem where neurons can become inactive during training.\n",
    "* **Parametric ReLU (PReLU)**:\n",
    "    * An extension of Leaky ReLU where  Î± is learned during training.\n",
    "* **Exponential Linear Unit (ELU)**:\n",
    "    * Smoothly saturates for negative inputs, potentially alleviating some issues with ReLU.\n",
    "    \n",
    "The choice of activation function depends on the specific characteristics of the data and the problem at hand. Experimentation and consideration of issues like vanishing gradients during training can guide the selection of an appropriate activation function for a given neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0ec45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
