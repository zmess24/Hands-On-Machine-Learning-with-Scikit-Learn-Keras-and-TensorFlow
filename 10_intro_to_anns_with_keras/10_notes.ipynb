{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf4dead",
   "metadata": {},
   "source": [
    "# Introduction to ANN's with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ea362",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron & Backpropogation\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a deep nueral network (dnn). The field of deep learning studies DNN's, and more generally models containing deep stacks of computations.\n",
    "\n",
    "ANN's and DNN's are trained via the `backpropogation` training algorithm, which in short is Gradient Descent using an efficient technique for computing the gradients automatically. The goal of backpropagation is to minimize the error between the predicted output of the network and the actual target values.\n",
    "\n",
    "The Backpropagation algorithm works by handling one mini-batch at a time (e.g, containing 32 training examples each) and goe through the full training set multiple times. Each pass is called an `epoch`. \n",
    "\n",
    "For each training instance the backpropogation algorith works by first making a prediction (forward pass) and measuring the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass) and finally tweaks the connection weights to reduce the error (gradient descent step).\n",
    "\n",
    "* **Initialization**: Initialize the weights and biases of the neural network with small random values. Define the learning rate, which determines the size of the steps taken during the optimization process.\n",
    "* **Forward Pass**: Each mini-batch is fed forward through the network layer by layer. Neurons in each layer perform a weighted sum of their inputs, add a bias, and then apply an activation function to produce the output.\n",
    "* **Calculate Error**: Compare the network's output with the actual target values to calculate the error. The error is typically measured using a loss or cost function, which quantifies the difference between the predicted and actual values.\n",
    "* **Backward Pass**: The goal is to then update the weights and biases in the network to reduce the error by calculateing the gradient of the error with respect to the weights and biases using the chain rule of calculus. This is done via propagating the gradient backward through the network until the algorithm reaches the input layer. to find how much each weight and bias contributed to the error.\n",
    "* **Update Weights and Biases**: Adjust the weights and biases in the direction that reduces the error. This is done by subtracting a fraction of the gradient multiplied by the learning rate. The learning rate determines the step size in the weight and bias updates. It's crucial to choose an appropriate learning rate to balance convergence speed and stability.\n",
    "* **Repeat**: Steps 2 to 5 are repeated for multiple epochs (passes through the entire training dataset) until the network's performance converges to an acceptable level. The backpropagation algorithm essentially iteratively adjusts the weights and biases of the neural network to minimize the error between predicted and actual outputs. This process is an optimization task, and the choice of the loss function, activation functions, and network architecture all play crucial roles in the success of the training process.\n",
    "\n",
    "In order for the algorithm to work properly, the authors made a key change by replacing the step function with the logistic (sigmoid) function. This was essential, because the step function contains only flat segment so there was no gradient to work with (GD doesn't work on a flat surface) while the logistic function has a well defined nonzero derivitiave everywhere, allowing GD to make some progress at every step. The logistic function is an example of an `activation function`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f94e36",
   "metadata": {},
   "source": [
    "### Activation Functions \n",
    "\n",
    "An activation function is a mathematical operation applied to the output of each neuron (or node) in a neural network layer. \n",
    "\n",
    "Activation functions introduces non-linearity to a DNN, allowing it to learn from and model complex patterns in data. Without non-linear activation functions, the entire neural network would behave like a linear model, regardless of its depth.\n",
    "\n",
    "The purpose of the activation function can be summarized as follows:\n",
    "\n",
    "* **Introducing Non-linearity**: Linear transformations (such as weighted sums and biases) are limited to representing linear relationships. By applying non-linear activation functions, the network can learn and approximate non-linear mappings between inputs and outputs.\n",
    "* **Enabling Complex Representations**: The stacking of non-linear activation functions in deep networks enables the modeling of intricate relationships and hierarchies in data, allowing the network to learn and represent complex patterns.\n",
    "\n",
    "Here are some commonly used activation functions in deep neural networks:\n",
    "\n",
    "* **Sigmoid Function (Logistic):**\n",
    "    * _MOST COMMON_\n",
    "    * Outputs values between 0 and 1.\n",
    "    * Historically used in the output layer for binary classification problems, but not as common in hidden layers due to the vanishing gradient problem.\n",
    "* **Hyperbolic Tangent (tanh)**:\n",
    "    * _MOST COMMON_\n",
    "    * Outputs values between -1 and 1.\n",
    "    * Similar to the sigmoid but with a higher output range.\n",
    "* **Rectified Linear Unit (ReLU)**:\n",
    "    * _MOST COMMON_\n",
    "    * Outputs zero for negative inputs and passes positive inputs as is.\n",
    "    * Widely used in hidden layers due to its simplicity and effectiveness in training deep networks.\n",
    "* **Leaky ReLU**:\n",
    "    * Similar to ReLU but allows a small, non-zero gradient for negative inputs, addressing the \"dying ReLU\" problem where neurons can become inactive during training.\n",
    "* **Parametric ReLU (PReLU)**:\n",
    "    * An extension of Leaky ReLU where  Î± is learned during training.\n",
    "* **Exponential Linear Unit (ELU)**:\n",
    "    * Smoothly saturates for negative inputs, potentially alleviating some issues with ReLU.\n",
    "    \n",
    "The choice of activation function depends on the specific characteristics of the data and the problem at hand. Experimentation and consideration of issues like vanishing gradients during training can guide the selection of an appropriate activation function for a given neural network architecture."
   ]
  },
  {
   "attachments": {
    "10-1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7820d068",
   "metadata": {},
   "source": [
    "## Regression MLP's\n",
    "\n",
    "If you want to predict a single value (e.g the price of a house, given many it's features) than you just need a single out neuron: it's output as the predicted value. For multivariate regression (e.g predicting multiple values at once), you need one ouput neuron per output dimension. \n",
    "\n",
    "In general, when an MLP for regression, you do not want to use any activation functino for the output neurons so they are free to output any rangeof values. \n",
    "\n",
    "* If you want to guarantee that the ouput will always be positive, than you can use the `ReLU` or `Softplus` (which is a smooth variant of `ReLU` activation functions in the ouput layer. \n",
    "* If you want to guarentee that the output falls within a range of values, than you can use `Logistic` or `Hyperbolic Tangent` activation functions and then scale the labels to the appropriate range: 0 to 1 of the `logistic` function and 0 to 1 for the `Hyperbolic Tangent`.\n",
    "\n",
    "The loss function to use during training is typically the MSE, but if you have a lot of outliers in the training set, you may prefer to use the MAE instead. You can also use Huber loss, which is a combination of both.\n",
    "\n",
    "![10-1.png](attachment:10-1.png)"
   ]
  },
  {
   "attachments": {
    "10-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a3d7099e",
   "metadata": {},
   "source": [
    "## Regression MLP's\n",
    "\n",
    "MLP's can also be used for classification tasks. For a binary classification problem, you will jsut need a single ouput neior using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of a given class. The estimated probability of the negative class is equal to 1 minus that number.\n",
    "\n",
    "MLP's can also easily handle multilabel binary classification tasks - for example, you could have an email classification system that predicts whether the incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons for that both using the logistic activation functions:\n",
    "\n",
    "* The first would predict the probability that email is spam.\n",
    "* The second would predict the probability that the email is urgent.\n",
    "\n",
    "If each instance can belong only to a single class (e.g **Multiclass Classification**, out of three or more possible classes (e.g classes 0 - 9 for digit image classification) then you need to have one ouput neuron per class, and you should use the `softmax` activation function for the whole output layer. The `softmax` function will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1, which is required if the classes are exclusive.\n",
    "\n",
    "Regarding the loss function, since we are predicting the probability distrubutions, the cross-entropy loss function is usually a good choice.\n",
    "\n",
    "![10-2.png](attachment:10-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772161e4",
   "metadata": {},
   "source": [
    "## Implementing MLPs w/ Keras\n",
    "\n",
    "Keras is a high-level Deep Learning API that allows you to easily build, train, evaluate and execute all sorts of nueral networks, and has been built into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b1d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.13.0\n",
      "Keras Version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe8a44",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f7e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7e2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (60000, 28, 28)\n",
      "Data Type: uint8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of data: {X_train_full.shape}\")\n",
    "print(f\"Data Type: {X_train_full.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff5068",
   "metadata": {},
   "source": [
    "Since we are going to scale the features using Gradient Descent, we need to scale the input features. For simplicity, we'll scale the the pixel intensities down to the 0-1 range by dividing them by 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb87d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0,\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75774e",
   "metadata": {},
   "source": [
    "Define the class names from 0 - 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d21b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce2337",
   "metadata": {},
   "source": [
    "### Creating the model using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46492581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b462e",
   "metadata": {},
   "source": [
    "* The first line creates a Sequential model, which is the simpliest kiund of Keras model for NN's that are just composed of a single stack of layers connected sequentially.\n",
    "* Next, we build the first layer and add it to the model. It is a `Flatten` layer whose role is to convert each input image into a 1D array: if it receives input data `X`, it computes `X.reshape(-1, 1)`. Since it is the first layer in the model, you should specifiy the `input_shape`, which doesn't inclue the batch size, only the shape of the instances. Alternatively, you could a `keras.layers.InputLayer(input_shape=[28,28])` as the first layer.\n",
    "* Next, we add a dense layer hidden layer with 300 neurons. It will use the `ReLU` activation function. Each Dense layer manages it's own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron).\n",
    "* Then we add a second Dense hidden layer with 100 neurons, also using the `ReLU` activation function.\n",
    "* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).\n",
    "\n",
    "Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the `Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbaff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9515c",
   "metadata": {},
   "source": [
    "The models `summary()` method displays nall the model's layers including each layers name (which is automatically generated unless you set it when creating the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffad9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e27be",
   "metadata": {},
   "source": [
    "Note that the Dense Lyers often a lot of parameters. For example, the first hidden layer has `784 * 300 (connection weights) + 300 (bias terms) = 235,500` parameters! This gives the model quite a lot flexability to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot training data.\n",
    "\n",
    "You can easily get a models list of layers, to fetch a layer by it's index, or fetch it by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf960515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.reshaping.flatten.Flatten at 0x14b383970>,\n",
       " <keras.src.layers.core.dense.Dense at 0x14b3831c0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x14c204550>,\n",
       " <keras.src.layers.core.dense.Dense at 0x10469e560>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752f3683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0fd8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cdf0f",
   "metadata": {},
   "source": [
    "All the weight parameters of a layer can be accessed using it's `get_weights()` and `set_weights()` methods. For Dense layers, this includes both the connection weights & bias terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4998ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05554695  0.01256878  0.03299689 ...  0.06764345  0.03037113\n",
      "  -0.05165726]\n",
      " [ 0.0423123  -0.0087942   0.00456925 ... -0.0200406  -0.06893791\n",
      "  -0.03823121]\n",
      " [ 0.01867315 -0.0179718  -0.05988745 ...  0.01291826  0.00085076\n",
      "   0.0675347 ]\n",
      " ...\n",
      " [-0.00365233 -0.03384567  0.03932574 ... -0.0281551   0.02965011\n",
      "   0.05090114]\n",
      " [ 0.04141407 -0.02151838  0.02786927 ...  0.02834836  0.01408258\n",
      "  -0.05446789]\n",
      " [-0.03158003 -0.0507543  -0.04054299 ...  0.00433651 -0.04742504\n",
      "  -0.03962693]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3f34a",
   "metadata": {},
   "source": [
    "Notice that the Dense layer initialized the connection wieghts randomly, which is need to break symmetry as we discussed earlier, and the biases were initialized to zeros. If you ever want to use a different initlaization method, you can set `kernal_initializer` or `bias_initi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd45924",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "After a model is created, you must call it's `compile()` method to specify the loss function and optimier to use. Optionally, you can also specify a lsit of extra metrics to compute during training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9182a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "loss=\"sparse_categorical_crossentropy\"\n",
    "optimizer=\"sgd\"\n",
    "metrics=[\"accuracy\"]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2e8d5",
   "metadata": {},
   "source": [
    "* We use `sparse_categorical_crossentropy`  for the loss function because we have sparse label (e.g for each instance there is just one target class) and the classes are exclusives. If instead we had one target probability per class for each instance (e.g a one hot vector), then we would need to use `categorical_crossentropy` loss function instead. If we were doing binary classifiction, than we would use the `binary_crossentropy` loss function.\n",
    "* Regarding the optimizer, `sgd` means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform backpropogation .\n",
    "* Finally, since this is classification, it's useful to measure it's accuracy durin training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb53de",
   "metadata": {},
   "source": [
    "### Training & Evaluating the Model\n",
    "\n",
    "Now the model is ready to be trained, which we can simply do using the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67742c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 820us/step - loss: 0.7086 - accuracy: 0.7667 - val_loss: 0.4983 - val_accuracy: 0.8342\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 1s 766us/step - loss: 0.4847 - accuracy: 0.8309 - val_loss: 0.4384 - val_accuracy: 0.8528\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 1s 770us/step - loss: 0.4409 - accuracy: 0.8454 - val_loss: 0.4198 - val_accuracy: 0.8600\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 1s 762us/step - loss: 0.4152 - accuracy: 0.8545 - val_loss: 0.4088 - val_accuracy: 0.8624\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 1s 762us/step - loss: 0.3949 - accuracy: 0.8608 - val_loss: 0.3776 - val_accuracy: 0.8708\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 1s 766us/step - loss: 0.3779 - accuracy: 0.8675 - val_loss: 0.3737 - val_accuracy: 0.8696\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 1s 768us/step - loss: 0.3643 - accuracy: 0.8717 - val_loss: 0.3663 - val_accuracy: 0.8734\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 1s 771us/step - loss: 0.3534 - accuracy: 0.8755 - val_loss: 0.3838 - val_accuracy: 0.8648\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 1s 779us/step - loss: 0.3431 - accuracy: 0.8768 - val_loss: 0.3433 - val_accuracy: 0.8782\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 1s 795us/step - loss: 0.3331 - accuracy: 0.8809 - val_loss: 0.3538 - val_accuracy: 0.8740\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 1s 801us/step - loss: 0.3240 - accuracy: 0.8855 - val_loss: 0.3408 - val_accuracy: 0.8786\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 1s 773us/step - loss: 0.3162 - accuracy: 0.8875 - val_loss: 0.3367 - val_accuracy: 0.8826\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 1s 777us/step - loss: 0.3093 - accuracy: 0.8892 - val_loss: 0.3465 - val_accuracy: 0.8776\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 1s 775us/step - loss: 0.3015 - accuracy: 0.8926 - val_loss: 0.3397 - val_accuracy: 0.8768\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 1s 781us/step - loss: 0.2962 - accuracy: 0.8928 - val_loss: 0.3174 - val_accuracy: 0.8856\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 1s 784us/step - loss: 0.2895 - accuracy: 0.8961 - val_loss: 0.3278 - val_accuracy: 0.8840\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 1s 786us/step - loss: 0.2845 - accuracy: 0.8985 - val_loss: 0.3189 - val_accuracy: 0.8850\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 1s 777us/step - loss: 0.2785 - accuracy: 0.9007 - val_loss: 0.3240 - val_accuracy: 0.8836\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 1s 783us/step - loss: 0.2733 - accuracy: 0.9023 - val_loss: 0.3161 - val_accuracy: 0.8838\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 1s 780us/step - loss: 0.2682 - accuracy: 0.9035 - val_loss: 0.3022 - val_accuracy: 0.8908\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 1s 778us/step - loss: 0.2638 - accuracy: 0.9057 - val_loss: 0.3048 - val_accuracy: 0.8870\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 1s 858us/step - loss: 0.2588 - accuracy: 0.9083 - val_loss: 0.3124 - val_accuracy: 0.8878\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 1s 785us/step - loss: 0.2544 - accuracy: 0.9092 - val_loss: 0.3034 - val_accuracy: 0.8874\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 1s 785us/step - loss: 0.2504 - accuracy: 0.9107 - val_loss: 0.3180 - val_accuracy: 0.8824\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 1s 772us/step - loss: 0.2448 - accuracy: 0.9117 - val_loss: 0.2958 - val_accuracy: 0.8922\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 1s 779us/step - loss: 0.2409 - accuracy: 0.9141 - val_loss: 0.2956 - val_accuracy: 0.8886\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 1s 776us/step - loss: 0.2374 - accuracy: 0.9147 - val_loss: 0.2993 - val_accuracy: 0.8950\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 1s 782us/step - loss: 0.2340 - accuracy: 0.9159 - val_loss: 0.3158 - val_accuracy: 0.8790\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 1s 778us/step - loss: 0.2293 - accuracy: 0.9187 - val_loss: 0.3152 - val_accuracy: 0.8864\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 1s 768us/step - loss: 0.2261 - accuracy: 0.9195 - val_loss: 0.2910 - val_accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab953978",
   "metadata": {},
   "source": [
    "The `fit()` method returns a `History` object containing the trianing parameters (`history.params`), the list of epochs it went through (`history.epoch`), and most importantly a dictionary (`history.history`) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any).\n",
    "\n",
    "You can use this dictionary to create a pandas DataFrame and call it's `plot()` method to get the learning curves shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c3bf8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af8948",
   "metadata": {},
   "source": [
    "You can see that both the training accuracy and the validation accuracy steadily increase during the training, while the training loss and validation loss decrease, which is good! Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting. Genearlly, the training set performance will end up beating the validation performance.\n",
    "\n",
    "You can tell that that the model has not queite converted yetm as the validation loss is still going down, meaning you should probably continue training. This is as simple as calling the `fit()` again, since Keras just continues training where it left off.\n",
    "\n",
    "If you are not satsified with the performance of the model, you should go back and tune the hyperparameters. The first one to check is teh learning rate, and if that doesn't help, try another optimizer (and always retune the learning after chaning any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the # of layers, the number of neuorns per layer, the types of activcation functions to use for each hidden layer, or the batch size.\n",
    "\n",
    "Once you are satisfied with your models validation accuracy, you should evaluate it on the test set to estiamte the generalization error before you deploy it to production via the `evaluate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c186486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 468us/step - loss: 54.9390 - accuracy: 0.8605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[54.93901824951172, 0.8604999780654907]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d55e5",
   "metadata": {},
   "source": [
    "It is common to get a slighly lower performance on the test set than on the validation set because the hyperparameters are tuned on the validation set and not the test set. Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estiamte of the generalization error will be too optimistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e50ad",
   "metadata": {},
   "source": [
    "### Using the Model to Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda87f7",
   "metadata": {},
   "source": [
    "Next, we can use the models `predict()` method to make predictions on new instances (since we don't have new instances, we will use the first instances of the initial test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff73746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd2c13",
   "metadata": {},
   "source": [
    "As you can see, for each instance the model estimates one probability per class, from 0-9. If you only care about the class with the highest estimated probability (even if that probability is quite low, than you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d8e23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Ankle Boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(y_proba,axis=1)\n",
    "print(y_pred)\n",
    "np.array(class_names)[y_pred]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86493343",
   "metadata": {},
   "source": [
    "Now you know how to use the sequential API to build, train, evaluate, and use a classification MLP. But what about regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510298",
   "metadata": {},
   "source": [
    "## Building a Regression MLP using the Sequential API\n",
    "\n",
    "Let's switch to the California housing problem and tackle it using a regression neural network. For simplicity, we will use Scikit-Learn's `fetch_california_housing()` function, since it contains only numerical values and there is no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edfa7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split the data into train, test, and validation splits\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29131b2e",
   "metadata": {},
   "source": [
    "Using the Sequential API to build, train, evaluate and use a regression MLP to make predictions is quite similar to what we did for classification. The main differences are: \n",
    "\n",
    "* The ouput layer has a single neuron, since we only want to proedcit a single value.\n",
    "* There is no activation function\n",
    "* The loss function is the MSE\n",
    "\n",
    "Since the dataset is quite noisy, we just use a single hidden layer with the fewer neurons than before to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18821f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.9795 - val_loss: 0.7471\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.4864 - val_loss: 0.5143\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 361us/step - loss: 0.4809 - val_loss: 0.4725\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.4276 - val_loss: 0.4358\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 365us/step - loss: 0.4082 - val_loss: 0.4254\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 359us/step - loss: 0.4008 - val_loss: 0.4198\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 364us/step - loss: 0.3956 - val_loss: 0.4245\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 360us/step - loss: 0.3918 - val_loss: 0.4123\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.3882 - val_loss: 0.4106\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 361us/step - loss: 0.3856 - val_loss: 0.4003\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 361us/step - loss: 0.3823 - val_loss: 0.3949\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 360us/step - loss: 0.3787 - val_loss: 0.3970\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3768 - val_loss: 0.3908\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.3753 - val_loss: 0.3896\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.3719 - val_loss: 0.3875\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.3707 - val_loss: 0.3843\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 361us/step - loss: 0.3683 - val_loss: 0.3815\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 364us/step - loss: 0.3665 - val_loss: 0.3827\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 364us/step - loss: 0.3647 - val_loss: 0.3774\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 364us/step - loss: 0.3639 - val_loss: 0.3705\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Complie the model\n",
    "loss=\"mean_squared_error\"\n",
    "optimizer=\"sgd\"\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b9b3bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a9b40a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 325us/step - loss: 0.3761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37606921792030334"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37e33563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predictions: [[3.5013378]\n",
      " [1.7640764]\n",
      " [1.0399872]]\n",
      "Labels: [3.074 1.489 0.986]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[:3] # Pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"Labels: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670b63c",
   "metadata": {},
   "source": [
    "As you can see, the Sequential API is easy to use. However, although `Sequential` models are extremely common, it is sometimes useful to build neural networks with more complex topologies or w/ multiple inputs or outputs. For this purpose, Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6dcb6",
   "metadata": {},
   "source": [
    "## Building Complex Models using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecd2ea",
   "metadata": {},
   "source": [
    "One example of a nonsequential NN is a `Wide & Deep` NN, which was introduced in 2016. It connects all or parts of the inputs directoly to the output layer, which makes it possible for the NN to learn both deep patterns (using the deep path) and simple rules (through the short path). \n",
    "\n",
    "In contrast, a regular MLP forces all the data to flow through teh full stack of layers, meaning simple patters in the data may end up getting distorted by this sequence. \n",
    "\n",
    "Let's build such a network using the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b911b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523a922",
   "metadata": {},
   "source": [
    "* First, we creat an input object. This is a specification of the kind of input the model wil lget, including it's `shape` and `dtype`. A model may actually have multiple inputs\n",
    "* Next, we create the `Dense` layer with 30 neurons, usinghte ReLU activation function. As soon as it's created, notice that we call it like a function, passing it to the input. This is why this is called teh **Functional API**. Note that we are just telling Keras how it should connect he layers together; no actual data is being processed yet.\n",
    "* We then create a second hidden layer, and again we use it as a function by passing it the output of the first hidden layer.\n",
    "* Next, we creat a `Concatenate` layer, and once again we immediately use it like a function to concatenate the input and the output of the second hidden layer. \n",
    "* Then we creat the output layer with a single neuron and no activation function and we call it like a function by passing it the result of the concatenation.\n",
    "* Lastly, we create a Keras model, specifiying which inputs and outputs to use.\n",
    "\n",
    "Once you have built the model, everything is exactly like earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46ad31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.8298 - val_loss: 2.0481\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.9100 - val_loss: 20.5099\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 374us/step - loss: 2.2171 - val_loss: 10.0465\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 371us/step - loss: 1.2136 - val_loss: 4.3973\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 367us/step - loss: 32.0905 - val_loss: 2.4728\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 365us/step - loss: 3.4652 - val_loss: 0.9537\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 401us/step - loss: 1.2781 - val_loss: 17.7942\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 420us/step - loss: 1.5650 - val_loss: 2.0111\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.5462 - val_loss: 0.6901\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 367us/step - loss: 1.0623 - val_loss: 0.4537\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.4303 - val_loss: 0.5121\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 0.5427 - val_loss: 0.4430\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.4126 - val_loss: 0.5447\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 0.4109 - val_loss: 0.4115\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.4047 - val_loss: 0.4125\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3902 - val_loss: 0.4191\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.4176 - val_loss: 0.4094\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3886 - val_loss: 0.4048\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3792 - val_loss: 0.3951\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 369us/step - loss: 0.3729 - val_loss: 0.4012\n"
     ]
    }
   ],
   "source": [
    "# Complie the model\n",
    "loss=\"mean_squared_error\"\n",
    "optimizer=\"sgd\"\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41731b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b76d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 309us/step - loss: 0.4015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40145212411880493"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee7e390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predictions: [[3.4267812]\n",
      " [1.5484924]\n",
      " [1.1961122]]\n",
      "Labels: [3.074 1.489 0.986]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[:3] # Pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"Labels: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3806d9b",
   "metadata": {},
   "source": [
    "But what if you want to send a subset of the features through the wide path and a different subset through the deep path? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4) and 6 features through the deep path (features 2 to 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcfba0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af7b65",
   "metadata": {},
   "source": [
    "Now we can compile the model as usual, but when the call the `fit()` method, instead of passing a single input matrix `X_train`, we must pass a paior of matrices (`X_train_A`, `X_train_B`): one per input. The same is true for `X_valid`, and also for `X_test` and `X_new` when you call `evaluate()` or `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8416887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 510us/step - loss: 1.7894 - val_loss: 0.8504\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.7496 - val_loss: 0.7110\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.6650 - val_loss: 0.6607\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.6236 - val_loss: 0.6267\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.5932 - val_loss: 0.5981\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 429us/step - loss: 0.5696 - val_loss: 0.5768\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.5490 - val_loss: 0.5578\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.5312 - val_loss: 0.5477\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 389us/step - loss: 0.5161 - val_loss: 0.5355\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.5034 - val_loss: 0.5183\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.4925 - val_loss: 0.5110\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 442us/step - loss: 0.4833 - val_loss: 0.5057\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 439us/step - loss: 0.4757 - val_loss: 0.4926\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.4683 - val_loss: 0.4894\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.4626 - val_loss: 0.4819\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.4569 - val_loss: 0.4801\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 390us/step - loss: 0.4523 - val_loss: 0.4723\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.4478 - val_loss: 0.4675\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.4439 - val_loss: 0.4629\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 390us/step - loss: 0.4403 - val_loss: 0.4567\n",
      "162/162 [==============================] - 0s 265us/step - loss: 0.4491\n",
      "MSE: 0.4491468369960785\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predictions: [[3.3838894]\n",
      " [1.531169 ]\n",
      " [1.2300739]]\n",
      "Labels: [3.074 1.489 0.986]\n"
     ]
    }
   ],
   "source": [
    "# Complile the model\n",
    "loss=\"mse\"\n",
    "optimizer=keras.optimizers.legacy.SGD(learning_rate=1e-3)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# Split Data into A & B groups\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "# Evalute MSE\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "print(f\"MSE: {mse_test}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"Labels: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23292516",
   "metadata": {},
   "source": [
    "There are many use cases in which you may want to have multiple outputs:\n",
    "\n",
    "* The task may demand it. For example, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the objects center, as well as it's width and height) and a classification task.\n",
    "* Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single NN with one output per task, because the NN can learn featuires in the data that are useful across tasks. For example, you could perform multitaks classification on picture of faces, using one output to classify the persons facial expression and another output to identify whether they are wearing glasses or not.\n",
    "* Another use case is as a regularization technique (e.g training constant whose objective is to reduce overfitting and thus improve the models ability to generalize). For example, you many want to add some auxiliary outputs in a NN's architecture to ensure that the underlying part of hte network learns something useful on its own w/out relying on the rest of the network.\n",
    "\n",
    "Extra outpus is quite easy: just connect them to the appropriate layers and add them to your models list of outputs. \n",
    "\n",
    "Each output will need it's own loss function, and therefore, when we compile the model, we should pass a list of losses (if you pass a single loss, Keras will assume thte same loss must be applied to all outputs). By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than the regularization output (as it is just used for regularization), so we want to give the mian output's loss a much greater weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0803a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=['mse', 'mse']\n",
    "loss_weights=[0.9, 0.1]\n",
    "optimizer=\"sgd\"\n",
    "\n",
    "model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc30658",
   "metadata": {},
   "source": [
    "Next, we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing `y_train`, we need to pass (`y_train`, `y_train`). The same goes for `y_valid` and `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18f95243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.3836 - val_loss: 0.3885\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3805 - val_loss: 0.3833\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3698 - val_loss: 0.3734\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3626 - val_loss: 0.3661\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3553 - val_loss: 0.3653\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3505 - val_loss: 0.3659\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3437 - val_loss: 0.3560\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3372 - val_loss: 0.3548\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3345 - val_loss: 0.3420\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3316 - val_loss: 0.3397\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3280 - val_loss: 0.3391\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3254 - val_loss: 0.3385\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3243 - val_loss: 0.3510\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3272 - val_loss: 0.3331\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3277 - val_loss: 0.3265\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.3170 - val_loss: 0.3260\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3159 - val_loss: 0.3277\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3150 - val_loss: 0.3238\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3135 - val_loss: 0.3323\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3109 - val_loss: 0.3207\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a457f24",
   "metadata": {},
   "source": [
    "When we evaluate the model, Keras will return the total loss, as well as the individual losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65d082",
   "metadata": {},
   "source": [
    "## Using the Subclassing APU to Build Dynamic Models\n",
    "\n",
    "Both the Sequential API and the Functional API are declaritive: you start by declaring which layers you want to use and how they should be connected, and only then can you start the feeding the model some data for training or inference.\n",
    "\n",
    "Having a static model like this has its advantages:\n",
    "\n",
    "* It's structure can be displayed and analyzed\n",
    "* It's easy to debug, and errors can be caught early\n",
    "* It can easily be saved, cloned, and shared\n",
    "\n",
    "However, some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, you may want to use a more imperative programming style by using the Subclassing API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "279d02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.max_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532cd75",
   "metadata": {},
   "source": [
    "The extra flexability of being able use added login in the `call` method comes at a cost - your model's architecture is hidden within the `call` method, Keras cannot easily inspect it, clone / save it, check shapes & types ahead of time, and debugging is harder.\n",
    "\n",
    "Unless you really need the extra flexibility, you should probably stick the the Sequential API or the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b8675",
   "metadata": {},
   "source": [
    "## Saving & Restoring a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7efbc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93f4ce",
   "metadata": {},
   "source": [
    "Saving a model is very easy - you will typically have a script that trains a model & saves it, and one or more scripts (or web service) that load the model and use it to make predictions. Loading the model is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39ff7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa0acd",
   "metadata": {},
   "source": [
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the `fit()` method to save the checkpoints? Use callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b96bb",
   "metadata": {},
   "source": [
    "## Using Callbacks\n",
    "\n",
    "The `fit()` method accepts a callbacks argument tha tlets you specify a list of all objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the `ModelCheckpoint` callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "402f489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 305us/step - loss: 3.6789\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 286us/step - loss: 0.4300\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 282us/step - loss: 0.3824\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 288us/step - loss: 0.3670\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 282us/step - loss: 0.3725\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 285us/step - loss: 0.3525\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 283us/step - loss: 0.3517\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 280us/step - loss: 0.3455\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 283us/step - loss: 0.3423\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 283us/step - loss: 0.3391\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Complie the model\n",
    "loss=\"mean_squared_error\"\n",
    "optimizer=\"sgd\"\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# Use callback to save model after each epoch\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad0699",
   "metadata": {},
   "source": [
    "If you use va validation set during training, you can set `save_best_only=True` when creating the `ModelCheckpoint`. In this case, it will only save your model when it's performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set - simply restore the last model save during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bdf19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.3356 - val_loss: 0.3371\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3336 - val_loss: 0.3410\n",
      "Epoch 3/10\n",
      "180/363 [=============>................] - ETA: 0s - loss: 0.3426"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 388us/step - loss: 0.3301 - val_loss: 0.3312\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3299 - val_loss: 0.3305\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.3262 - val_loss: 0.3266\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3266 - val_loss: 0.3265\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3244 - val_loss: 0.3306\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3218 - val_loss: 0.3301\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3223 - val_loss: 0.3352\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 385us/step - loss: 0.3214 - val_loss: 0.3234\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "\n",
    "# Roll back to the best model\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba702f3",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the `EarlyStopping` callback. It will interrupt training when it measures no progress on the validation set for a # of epochs (defined by the `patience` arugment) and it will optionally roll back to the best argument.\n",
    "\n",
    "You can combine both callbacks to save checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid asting time & resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e4db0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 442us/step - loss: 0.3126 - val_loss: 0.3162\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.3126 - val_loss: 0.3139\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.3172 - val_loss: 0.3141\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3119 - val_loss: 0.3149\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3112 - val_loss: 0.3177\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3101 - val_loss: 0.3180\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3105 - val_loss: 0.3120\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3092 - val_loss: 0.3125\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3089 - val_loss: 0.3158\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3073 - val_loss: 0.3368\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3087 - val_loss: 0.3157\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3091 - val_loss: 0.3146\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3082 - val_loss: 0.3189\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3091 - val_loss: 0.3095\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3064 - val_loss: 0.3197\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3077 - val_loss: 0.3176\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3091 - val_loss: 0.3230\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3076 - val_loss: 0.3163\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3062 - val_loss: 0.3174\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3046 - val_loss: 0.3096\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3061 - val_loss: 0.3123\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.3049 - val_loss: 0.3080\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 369us/step - loss: 0.3142 - val_loss: 0.3158\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3050 - val_loss: 0.3084\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3044 - val_loss: 0.3142\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3037 - val_loss: 0.3136\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3028 - val_loss: 0.3091\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3061 - val_loss: 0.3116\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3033 - val_loss: 0.3123\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3027 - val_loss: 0.3095\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3041 - val_loss: 0.3099\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3061 - val_loss: 0.3054\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3024 - val_loss: 0.3057\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3015 - val_loss: 0.3389\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3004 - val_loss: 0.3098\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3017 - val_loss: 0.3124\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3010 - val_loss: 0.3157\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3022 - val_loss: 0.3073\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 369us/step - loss: 0.3027 - val_loss: 0.3121\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3000 - val_loss: 0.3119\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.2990 - val_loss: 0.3196\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3006 - val_loss: 0.3124\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304b663",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d03fabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389b2c3",
   "metadata": {},
   "source": [
    "## Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde7a9d",
   "metadata": {},
   "source": [
    "TensorBoard is ag rat interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graphh, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clusterd for you, and more! This tool is installed automatically when you install TensorFlow, so you have already have it. \n",
    "\n",
    "To use it, you must modify your program so that it ouputs the data you want to visualize to special binary log files called _event files_. Let's start by defining the root log directory wwe will use for our TensorBoard logs, plus a small function that will generate a subdirectory path based on the current data an time so that's different every run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88b062aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() #e.g, './my_logs/run_2019_06_07_15_15_22'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8dbdb0",
   "metadata": {},
   "source": [
    "The good news is that Keras provides a nice `TensorBoard()` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81a003fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 464us/step - loss: 0.2870 - val_loss: 0.3012\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.2865 - val_loss: 0.3016\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.2862 - val_loss: 0.3048\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 432us/step - loss: 0.2866 - val_loss: 0.3024\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.2852 - val_loss: 0.3013\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.2858 - val_loss: 0.3090\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.2859 - val_loss: 0.3090\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 410us/step - loss: 0.2853 - val_loss: 0.3504\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.2917 - val_loss: 0.3157\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.2873 - val_loss: 0.3032\n"
     ]
    }
   ],
   "source": [
    "# [...] Build & Compile Model\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5fad9",
   "metadata": {},
   "source": [
    "Next, you need to start the TensorBoard server. This can be done directly within Jupyter by runing the following commands - the first line loads the TensorBoard extension, and the second line starts a TensorBoard server on port 6006 and connects to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "883bf4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b537f489d150058\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b537f489d150058\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd8d91",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1191c9c",
   "metadata": {},
   "source": [
    "The flexability of NN's is also one of it's biggest drawbacks - there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple MLP you can change the # of layers, the # of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do you know what combination of hyperparameters is best for your task?\n",
    "\n",
    "One option is to simply try as many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross validation). For example we could use `GridSearchCV` or `RandomizedSearchCV` to explore the hyperparameter space. The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f6e1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    # Init Model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Create Input Layers\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Create Hidden Layers\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    \n",
    "    # Output Layer    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Compile Model\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f230d",
   "metadata": {},
   "source": [
    "Next, let's create a `KerasRegressor` based on this `build_model()` function, since we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d1aef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-metal<2.0.0,>=1.1.0\n",
      "  Downloading tensorflow_metal-1.1.0-cp310-cp310-macosx_12_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=0.21 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikeras) (22.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikeras) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.1.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from tensorflow-metal<2.0.0,>=1.1.0->scikeras) (0.38.4)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from tensorflow-metal<2.0.0,>=1.1.0->scikeras) (1.16.0)\n",
      "Installing collected packages: tensorflow-metal, scikeras\n",
      "Successfully installed scikeras-0.12.0 tensorflow-metal-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "keras_reg = KerasRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2b68a",
   "metadata": {},
   "source": [
    "Now we can use this object like a regular Scikit-Learn regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de0c8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 475us/step - loss: 2.2715 - val_loss: 0.5631\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.4810 - val_loss: 0.4227\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3985 - val_loss: 0.3960\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3801 - val_loss: 0.3865\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3722 - val_loss: 0.3828\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3664 - val_loss: 0.3837\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3626 - val_loss: 0.3718\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 488us/step - loss: 0.3588 - val_loss: 0.3700\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 389us/step - loss: 0.3570 - val_loss: 0.3706\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3546 - val_loss: 0.3645\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.3525 - val_loss: 0.3672\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.3508 - val_loss: 0.3648\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3500 - val_loss: 0.3642\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3475 - val_loss: 0.3598\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3453 - val_loss: 0.3589\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3432 - val_loss: 0.3597\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3430 - val_loss: 0.3582\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3415 - val_loss: 0.3562\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3396 - val_loss: 0.3539\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3385 - val_loss: 0.3493\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3378 - val_loss: 0.3493\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3372 - val_loss: 0.3520\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 432us/step - loss: 0.3358 - val_loss: 0.3514\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3352 - val_loss: 0.3577\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3340 - val_loss: 0.3573\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3337 - val_loss: 0.3438\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.3324 - val_loss: 0.3481\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3313 - val_loss: 0.3472\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 362us/step - loss: 0.3305 - val_loss: 0.3571\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3304 - val_loss: 0.3435\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3283 - val_loss: 0.3449\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3283 - val_loss: 0.3419\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3279 - val_loss: 0.3423\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3270 - val_loss: 0.3446\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3266 - val_loss: 0.3474\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3260 - val_loss: 0.3412\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3241 - val_loss: 0.3455\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3241 - val_loss: 0.3391\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3227 - val_loss: 0.3320\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 359us/step - loss: 0.3229 - val_loss: 0.3381\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3219 - val_loss: 0.3374\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3232 - val_loss: 0.3386\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3210 - val_loss: 0.3293\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3203 - val_loss: 0.3428\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3200 - val_loss: 0.3314\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3199 - val_loss: 0.3306\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3186 - val_loss: 0.3292\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3190 - val_loss: 0.3321\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3175 - val_loss: 0.3246\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3194 - val_loss: 0.3325\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3191 - val_loss: 0.3313\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3172 - val_loss: 0.3374\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3170 - val_loss: 0.3288\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3173 - val_loss: 0.3309\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3158 - val_loss: 0.3324\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3146 - val_loss: 0.3248\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3150 - val_loss: 0.3215\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3160 - val_loss: 0.3316\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3146 - val_loss: 0.3259\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3147 - val_loss: 0.3383\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3150 - val_loss: 0.3239\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3141 - val_loss: 0.3227\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3117 - val_loss: 0.3259\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3120 - val_loss: 0.3246\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3124 - val_loss: 0.3244\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3133 - val_loss: 0.3222\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3125 - val_loss: 0.3717\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3142 - val_loss: 0.3225\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3117 - val_loss: 0.3246\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3099 - val_loss: 0.3386\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3117 - val_loss: 0.3170\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3105 - val_loss: 0.3207\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3099 - val_loss: 0.3354\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3094 - val_loss: 0.3203\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 374us/step - loss: 0.3108 - val_loss: 0.3171\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3104 - val_loss: 0.3375\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3105 - val_loss: 0.3188\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3109 - val_loss: 0.3206\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3098 - val_loss: 0.3168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3097 - val_loss: 0.3209\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 365us/step - loss: 0.3089 - val_loss: 0.3177\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3085 - val_loss: 0.3277\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3129 - val_loss: 0.3178\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3105 - val_loss: 0.3214\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3128 - val_loss: 0.3153\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 363us/step - loss: 0.3075 - val_loss: 0.3153\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3073 - val_loss: 0.3148\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3071 - val_loss: 0.3220\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 365us/step - loss: 0.3064 - val_loss: 0.3241\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3062 - val_loss: 0.3171\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 370us/step - loss: 0.3068 - val_loss: 0.3228\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3059 - val_loss: 0.3161\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.3068 - val_loss: 0.3237\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 368us/step - loss: 0.3058 - val_loss: 0.3133\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 365us/step - loss: 0.3051 - val_loss: 0.3173\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 371us/step - loss: 0.3052 - val_loss: 0.3124\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3050 - val_loss: 0.3142\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 366us/step - loss: 0.3039 - val_loss: 0.3215\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 367us/step - loss: 0.3039 - val_loss: 0.3197\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 365us/step - loss: 0.3059 - val_loss: 0.3144\n",
      "162/162 [==============================] - 0s 228us/step\n",
      "1/1 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[earlystopping_cb])\n",
    "\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c072e",
   "metadata": {},
   "source": [
    "We don't want to train and evaulate a single model like this - we want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search.\n",
    "\n",
    "Let's try to explore teh # of hidden layers, the number of neurons, and the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1757db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/zacharymessinger/anaconda3/lib/python3.10/site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be3eae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 591us/step - loss: 1.2628 - val_loss: 0.6743\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.5773 - val_loss: 0.5332\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4822 - val_loss: 0.4741\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4414 - val_loss: 0.4468\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.4198 - val_loss: 0.4298\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4069 - val_loss: 0.4169\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3961 - val_loss: 0.4078\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3884 - val_loss: 0.3949\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3823 - val_loss: 0.3933\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3767 - val_loss: 0.3860\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3714 - val_loss: 0.3815\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3669 - val_loss: 0.3729\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3645 - val_loss: 0.3723\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3599 - val_loss: 0.3753\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3566 - val_loss: 0.3719\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3547 - val_loss: 0.3615\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3529 - val_loss: 0.3597\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3484 - val_loss: 0.3583\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3468 - val_loss: 0.3588\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3455 - val_loss: 0.3546\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3484 - val_loss: 0.3576\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3421 - val_loss: 0.3526\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3424 - val_loss: 0.3724\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3414 - val_loss: 0.3462\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3389 - val_loss: 0.3659\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3377 - val_loss: 0.3517\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3393 - val_loss: 0.3443\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3344 - val_loss: 0.3421\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3338 - val_loss: 0.3435\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3319 - val_loss: 0.3389\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3318 - val_loss: 0.3438\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3306 - val_loss: 0.3372\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3288 - val_loss: 0.3508\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3291 - val_loss: 0.3552\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3282 - val_loss: 0.3410\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3252 - val_loss: 0.3573\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3270 - val_loss: 0.3562\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3359 - val_loss: 0.3317\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3252 - val_loss: 0.3411\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3236 - val_loss: 0.3354\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3248 - val_loss: 0.3323\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3226 - val_loss: 0.3412\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3216 - val_loss: 0.3469\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3215 - val_loss: 0.3323\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3212 - val_loss: 0.3313\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3216 - val_loss: 0.3304\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3201 - val_loss: 0.3365\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3200 - val_loss: 0.3304\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3199 - val_loss: 0.3394\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3201 - val_loss: 0.3325\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.3185 - val_loss: 0.3318\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3183 - val_loss: 0.3347\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3187 - val_loss: 0.3340\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3175 - val_loss: 0.3249\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3159 - val_loss: 0.3308\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3154 - val_loss: 0.3298\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3150 - val_loss: 0.3263\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3156 - val_loss: 0.3377\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3170 - val_loss: 0.3254\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3146 - val_loss: 0.3351\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3126 - val_loss: 0.3229\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3134 - val_loss: 0.3251\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3132 - val_loss: 0.3286\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3117 - val_loss: 0.3279\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3122 - val_loss: 0.3230\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3136 - val_loss: 0.3278\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3125 - val_loss: 0.3253\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3131 - val_loss: 0.3216\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3128 - val_loss: 0.3219\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3137 - val_loss: 0.3229\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3104 - val_loss: 0.3229\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3115 - val_loss: 0.3213\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3103 - val_loss: 0.3344\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3096 - val_loss: 0.3225\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3096 - val_loss: 0.3183\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3088 - val_loss: 0.3233\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3084 - val_loss: 0.3186\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3091 - val_loss: 0.3199\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3074 - val_loss: 0.3204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3077 - val_loss: 0.3192\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3080 - val_loss: 0.3215\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3063 - val_loss: 0.3206\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3062 - val_loss: 0.3214\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3057 - val_loss: 0.3154\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3063 - val_loss: 0.3247\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3060 - val_loss: 0.3176\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3061 - val_loss: 0.3179\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3058 - val_loss: 0.3171\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3050 - val_loss: 0.3213\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3041 - val_loss: 0.3186\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3045 - val_loss: 0.3144\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3037 - val_loss: 0.3168\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3048 - val_loss: 0.3172\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3030 - val_loss: 0.3213\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3030 - val_loss: 0.3189\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3028 - val_loss: 0.3202\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3015 - val_loss: 0.3159\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3031 - val_loss: 0.3148\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3029 - val_loss: 0.3182\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3012 - val_loss: 0.3164\n",
      "121/121 [==============================] - 0s 233us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 1.0830 - val_loss: 0.9737\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.8316 - val_loss: 0.7046\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.5722 - val_loss: 0.4869\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4338 - val_loss: 0.4544\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4127 - val_loss: 0.4389\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3990 - val_loss: 0.4268\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3894 - val_loss: 0.4173\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3826 - val_loss: 0.4128\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3769 - val_loss: 0.4086\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3721 - val_loss: 0.4099\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3673 - val_loss: 0.3999\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3645 - val_loss: 0.3994\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3630 - val_loss: 0.4005\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3603 - val_loss: 0.3914\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3575 - val_loss: 0.3953\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3549 - val_loss: 0.3885\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3526 - val_loss: 0.3819\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3508 - val_loss: 0.3878\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3500 - val_loss: 0.3784\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3474 - val_loss: 0.3791\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3445 - val_loss: 0.3742\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3438 - val_loss: 0.3757\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3419 - val_loss: 0.3700\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3405 - val_loss: 0.3709\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3403 - val_loss: 0.3714\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3379 - val_loss: 0.3657\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3360 - val_loss: 0.3620\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3344 - val_loss: 0.3652\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3334 - val_loss: 0.3614\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3320 - val_loss: 0.3666\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3307 - val_loss: 0.3625\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 561us/step - loss: 0.3301 - val_loss: 0.3596\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3281 - val_loss: 0.3571\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3270 - val_loss: 0.3523\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.3261 - val_loss: 0.3541\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3254 - val_loss: 0.3475\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3237 - val_loss: 0.3500\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3235 - val_loss: 0.3537\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3222 - val_loss: 0.3492\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3212 - val_loss: 0.3480\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3211 - val_loss: 0.3419\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3180 - val_loss: 0.3471\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3168 - val_loss: 0.3472\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3187 - val_loss: 0.3485\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3181 - val_loss: 0.3437\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3167 - val_loss: 0.3467\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3158 - val_loss: 0.3386\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3147 - val_loss: 0.3437\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3139 - val_loss: 0.3395\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3133 - val_loss: 0.3384\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3125 - val_loss: 0.3369\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3123 - val_loss: 0.3481\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3123 - val_loss: 0.3330\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3108 - val_loss: 0.3437\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3118 - val_loss: 0.3364\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3109 - val_loss: 0.3399\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3102 - val_loss: 0.3424\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3103 - val_loss: 0.3353\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3094 - val_loss: 0.3361\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3088 - val_loss: 0.3358\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3092 - val_loss: 0.3322\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3082 - val_loss: 0.3346\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3071 - val_loss: 0.3448\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3072 - val_loss: 0.3395\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3065 - val_loss: 0.3362\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3074 - val_loss: 0.3413\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3068 - val_loss: 0.3399\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3058 - val_loss: 0.3400\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3065 - val_loss: 0.3252\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3049 - val_loss: 0.3276\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3054 - val_loss: 0.3317\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3050 - val_loss: 0.3342\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3055 - val_loss: 0.3261\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3038 - val_loss: 0.3311\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3038 - val_loss: 0.3275\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3017 - val_loss: 0.3518\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3025 - val_loss: 0.3261\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3025 - val_loss: 0.3190\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3027 - val_loss: 0.3231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3012 - val_loss: 0.3197\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3019 - val_loss: 0.3180\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3012 - val_loss: 0.3316\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2995 - val_loss: 0.3296\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3003 - val_loss: 0.3175\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.2985 - val_loss: 0.3310\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2991 - val_loss: 0.3195\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3009 - val_loss: 0.3229\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2985 - val_loss: 0.3249\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2976 - val_loss: 0.3165\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2974 - val_loss: 0.3201\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2980 - val_loss: 0.3205\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2955 - val_loss: 0.3252\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2958 - val_loss: 0.3207\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.2969 - val_loss: 0.3201\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2956 - val_loss: 0.3282\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2956 - val_loss: 0.3175\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2956 - val_loss: 0.3236\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2943 - val_loss: 0.3138\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2939 - val_loss: 0.3227\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2931 - val_loss: 0.3135\n",
      "121/121 [==============================] - 0s 243us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.8524 - val_loss: 0.6593\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.5752 - val_loss: 0.5507\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4955 - val_loss: 0.4854\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.4545 - val_loss: 0.4526\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.4275 - val_loss: 0.4400\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4129 - val_loss: 0.4279\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4015 - val_loss: 0.4130\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3944 - val_loss: 0.4081\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3874 - val_loss: 0.4005\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3815 - val_loss: 0.3905\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3774 - val_loss: 0.3924\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3779 - val_loss: 0.3887\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3724 - val_loss: 0.3842\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3693 - val_loss: 0.3789\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3637 - val_loss: 0.3814\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3627 - val_loss: 0.3786\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3623 - val_loss: 0.3892\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3571 - val_loss: 0.3818\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3581 - val_loss: 0.3773\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3577 - val_loss: 0.3685\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3542 - val_loss: 0.3690\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3519 - val_loss: 0.3617\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3512 - val_loss: 0.3628\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3494 - val_loss: 0.3654\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3454 - val_loss: 0.3577\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3458 - val_loss: 0.3620\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3437 - val_loss: 0.3613\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3430 - val_loss: 0.3591\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3403 - val_loss: 0.3544\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3413 - val_loss: 0.3798\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3381 - val_loss: 0.3559\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3353 - val_loss: 0.3602\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3347 - val_loss: 0.3558\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3332 - val_loss: 0.3508\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3327 - val_loss: 0.3470\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3329 - val_loss: 0.3709\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3305 - val_loss: 0.3454\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3254 - val_loss: 0.3496\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3269 - val_loss: 0.3421\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3268 - val_loss: 0.3402\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3256 - val_loss: 0.3389\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3237 - val_loss: 0.3508\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3223 - val_loss: 0.3415\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3203 - val_loss: 0.3391\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3191 - val_loss: 0.3310\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3195 - val_loss: 0.3345\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3190 - val_loss: 0.3371\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3176 - val_loss: 0.3505\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3176 - val_loss: 0.3300\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3160 - val_loss: 0.3270\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3145 - val_loss: 0.3267\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3137 - val_loss: 0.3290\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3125 - val_loss: 0.3335\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3130 - val_loss: 0.3321\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3127 - val_loss: 0.3363\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3143 - val_loss: 0.3204\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3115 - val_loss: 0.3246\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3087 - val_loss: 0.3221\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3107 - val_loss: 0.3175\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3070 - val_loss: 0.3271\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3073 - val_loss: 0.3272\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3056 - val_loss: 0.3144\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3057 - val_loss: 0.3194\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3074 - val_loss: 0.3156\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3036 - val_loss: 0.3206\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3042 - val_loss: 0.3139\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3034 - val_loss: 0.3302\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3029 - val_loss: 0.3131\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3014 - val_loss: 0.3146\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3002 - val_loss: 0.3210\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3003 - val_loss: 0.3165\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3003 - val_loss: 0.3183\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2997 - val_loss: 0.3127\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2984 - val_loss: 0.3194\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2954 - val_loss: 0.3258\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2983 - val_loss: 0.3105\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2976 - val_loss: 0.3136\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2979 - val_loss: 0.3204\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2979 - val_loss: 0.3111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2948 - val_loss: 0.3132\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2965 - val_loss: 0.3128\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2953 - val_loss: 0.3129\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.2948 - val_loss: 0.3062\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.2943 - val_loss: 0.3060\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2920 - val_loss: 0.3057\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2929 - val_loss: 0.3041\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 444us/step - loss: 0.2934 - val_loss: 0.3092\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.2927 - val_loss: 0.3074\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.2917 - val_loss: 0.3100\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2916 - val_loss: 0.3119\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2909 - val_loss: 0.3121\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2909 - val_loss: 0.3115\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2902 - val_loss: 0.3269\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2911 - val_loss: 0.3123\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2894 - val_loss: 0.3125\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2894 - val_loss: 0.3179\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2900 - val_loss: 0.3097\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2880 - val_loss: 0.3065\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2878 - val_loss: 0.3125\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.2888 - val_loss: 0.3065\n",
      "121/121 [==============================] - 0s 244us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.9410 - val_loss: 0.6017\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.5269 - val_loss: 0.5095\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4738 - val_loss: 0.4789\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4435 - val_loss: 0.4415\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4203 - val_loss: 0.4165\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4052 - val_loss: 0.4136\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3938 - val_loss: 0.3966\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3842 - val_loss: 0.3978\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3780 - val_loss: 0.3823\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3733 - val_loss: 0.3774\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3668 - val_loss: 0.3737\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3632 - val_loss: 0.3844\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3602 - val_loss: 0.3701\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3593 - val_loss: 0.3648\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3530 - val_loss: 0.3613\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3511 - val_loss: 0.3547\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3493 - val_loss: 0.3620\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3471 - val_loss: 0.3637\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3456 - val_loss: 0.3604\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3417 - val_loss: 0.3548\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3421 - val_loss: 0.3893\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3393 - val_loss: 0.3571\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3402 - val_loss: 0.3412\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3366 - val_loss: 0.3483\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3366 - val_loss: 0.3451\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3321 - val_loss: 0.3459\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3288 - val_loss: 0.3406\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3277 - val_loss: 0.3387\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3289 - val_loss: 0.3395\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3259 - val_loss: 0.3345\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3235 - val_loss: 0.3539\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3214 - val_loss: 0.3426\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3218 - val_loss: 0.3442\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3195 - val_loss: 0.3354\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3173 - val_loss: 0.3298\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3176 - val_loss: 0.3314\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3194 - val_loss: 0.3349\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3153 - val_loss: 0.3330\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3187 - val_loss: 0.3278\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3131 - val_loss: 0.3219\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3167 - val_loss: 0.3259\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3108 - val_loss: 0.3316\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3111 - val_loss: 0.3227\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3083 - val_loss: 0.3264\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3101 - val_loss: 0.3207\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3068 - val_loss: 0.3369\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3114 - val_loss: 0.3201\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3124 - val_loss: 0.3276\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3034 - val_loss: 0.3312\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3057 - val_loss: 0.3289\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3018 - val_loss: 0.3159\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3028 - val_loss: 0.3145\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3009 - val_loss: 0.3203\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3013 - val_loss: 0.3270\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3002 - val_loss: 0.3158\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3003 - val_loss: 0.3176\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3007 - val_loss: 0.3149\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2987 - val_loss: 0.3137\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2972 - val_loss: 0.3196\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2965 - val_loss: 0.3130\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2970 - val_loss: 0.3174\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2948 - val_loss: 0.3171\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2951 - val_loss: 0.3165\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2936 - val_loss: 0.3280\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2911 - val_loss: 0.3136\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2937 - val_loss: 0.3127\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2926 - val_loss: 0.3096\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2932 - val_loss: 0.3092\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2913 - val_loss: 0.3099\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2887 - val_loss: 0.3103\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2911 - val_loss: 0.3089\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2891 - val_loss: 0.3210\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2906 - val_loss: 0.3081\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2888 - val_loss: 0.3085\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2882 - val_loss: 0.3106\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2897 - val_loss: 0.3129\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2885 - val_loss: 0.3051\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2860 - val_loss: 0.3128\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2869 - val_loss: 0.3118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.2848 - val_loss: 0.3080\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2853 - val_loss: 0.3137\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2862 - val_loss: 0.3070\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2861 - val_loss: 0.3032\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.2838 - val_loss: 0.3076\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2851 - val_loss: 0.2996\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2850 - val_loss: 0.2987\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2835 - val_loss: 0.3168\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2829 - val_loss: 0.2999\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2827 - val_loss: 0.3060\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.2816 - val_loss: 0.3039\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2819 - val_loss: 0.3019\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2807 - val_loss: 0.3008\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.2822 - val_loss: 0.2984\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2808 - val_loss: 0.3004\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2802 - val_loss: 0.3049\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2821 - val_loss: 0.3074\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2814 - val_loss: 0.3058\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2778 - val_loss: 0.3028\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2797 - val_loss: 0.2988\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.2776 - val_loss: 0.3046\n",
      "121/121 [==============================] - 0s 271us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 625us/step - loss: 1.3845 - val_loss: 0.7464\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 448us/step - loss: 1.0385 - val_loss: 0.5836\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.4579 - val_loss: 0.4350\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3983 - val_loss: 0.4050\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3789 - val_loss: 0.3916\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3676 - val_loss: 0.3857\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3590 - val_loss: 0.3820\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3532 - val_loss: 0.3749\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3471 - val_loss: 0.3822\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3441 - val_loss: 0.3757\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3404 - val_loss: 0.3647\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3378 - val_loss: 0.3592\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3355 - val_loss: 0.3588\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3324 - val_loss: 0.3564\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3284 - val_loss: 0.3539\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3259 - val_loss: 0.3570\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3248 - val_loss: 0.3513\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3230 - val_loss: 0.3451\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3205 - val_loss: 0.3434\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3188 - val_loss: 0.3502\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3169 - val_loss: 0.3456\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3167 - val_loss: 0.3392\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3136 - val_loss: 0.3395\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3128 - val_loss: 0.3420\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3111 - val_loss: 0.3388\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3106 - val_loss: 0.3446\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3075 - val_loss: 0.3438\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3072 - val_loss: 0.3328\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3054 - val_loss: 0.3362\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3045 - val_loss: 0.3335\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3036 - val_loss: 0.3291\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3020 - val_loss: 0.3290\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3015 - val_loss: 0.3271\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 442us/step - loss: 0.3009 - val_loss: 0.3282\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 451us/step - loss: 0.3006 - val_loss: 0.3357\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 448us/step - loss: 0.2988 - val_loss: 0.3246\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.2982 - val_loss: 0.3416\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.2970 - val_loss: 0.3335\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2959 - val_loss: 0.3252\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2960 - val_loss: 0.3216\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2945 - val_loss: 0.3221\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2931 - val_loss: 0.3280\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2930 - val_loss: 0.3230\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2928 - val_loss: 0.3171\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2921 - val_loss: 0.3222\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2910 - val_loss: 0.3181\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2901 - val_loss: 0.3232\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2892 - val_loss: 0.3146\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2892 - val_loss: 0.3220\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2888 - val_loss: 0.3347\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2887 - val_loss: 0.3164\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2881 - val_loss: 0.3151\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.2869 - val_loss: 0.3165\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2855 - val_loss: 0.3189\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2857 - val_loss: 0.3139\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2864 - val_loss: 0.3163\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2849 - val_loss: 0.3123\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2833 - val_loss: 0.3182\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2834 - val_loss: 0.3114\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2834 - val_loss: 0.3166\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 446us/step - loss: 0.2830 - val_loss: 0.3155\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2822 - val_loss: 0.3173\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2828 - val_loss: 0.3173\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2817 - val_loss: 0.3185\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2812 - val_loss: 0.3271\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2810 - val_loss: 0.3121\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2804 - val_loss: 0.3211\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2813 - val_loss: 0.3088\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2803 - val_loss: 0.3115\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2792 - val_loss: 0.3089\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2800 - val_loss: 0.3169\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2789 - val_loss: 0.3134\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2788 - val_loss: 0.3117\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2772 - val_loss: 0.3085\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2778 - val_loss: 0.3195\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2798 - val_loss: 0.3086\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2757 - val_loss: 0.3136\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2770 - val_loss: 0.3071\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2773 - val_loss: 0.3134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2769 - val_loss: 0.3088\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2760 - val_loss: 0.3148\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2774 - val_loss: 0.3107\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2761 - val_loss: 0.3111\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.2749 - val_loss: 0.3103\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2752 - val_loss: 0.3266\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2745 - val_loss: 0.3136\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2749 - val_loss: 0.3053\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2737 - val_loss: 0.3061\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2737 - val_loss: 0.3074\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2746 - val_loss: 0.3113\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2734 - val_loss: 0.3084\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2736 - val_loss: 0.3057\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2718 - val_loss: 0.3101\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.2736 - val_loss: 0.3125\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.2731 - val_loss: 0.3226\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2720 - val_loss: 0.3154\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2722 - val_loss: 0.3167\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2719 - val_loss: 0.3047\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2728 - val_loss: 0.3162\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2717 - val_loss: 0.3213\n",
      "121/121 [==============================] - 0s 272us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.7958 - val_loss: 0.6545\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.5554 - val_loss: 0.5453\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4788 - val_loss: 0.4931\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4364 - val_loss: 0.4632\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4108 - val_loss: 0.4299\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3942 - val_loss: 0.4111\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3892 - val_loss: 0.4015\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3811 - val_loss: 0.3890\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3757 - val_loss: 0.3823\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3690 - val_loss: 0.4295\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3674 - val_loss: 0.3804\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3595 - val_loss: 0.3750\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3561 - val_loss: 0.3669\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3544 - val_loss: 0.3732\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3520 - val_loss: 0.3648\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3473 - val_loss: 0.3694\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3459 - val_loss: 0.3721\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3439 - val_loss: 0.3531\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3412 - val_loss: 0.3494\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3359 - val_loss: 0.3533\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3342 - val_loss: 0.3550\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3331 - val_loss: 0.3483\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3319 - val_loss: 0.3533\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3290 - val_loss: 0.3432\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3256 - val_loss: 0.3434\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3222 - val_loss: 0.3406\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3228 - val_loss: 0.3464\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3197 - val_loss: 0.3351\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3192 - val_loss: 0.3371\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3162 - val_loss: 0.3321\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3153 - val_loss: 0.3311\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3137 - val_loss: 0.3321\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3129 - val_loss: 0.3352\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3117 - val_loss: 0.3285\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3137 - val_loss: 0.3263\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3088 - val_loss: 0.3649\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3089 - val_loss: 0.3287\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3076 - val_loss: 0.3299\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3066 - val_loss: 0.3238\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3081 - val_loss: 0.3235\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3068 - val_loss: 0.3223\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3040 - val_loss: 0.3158\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3037 - val_loss: 0.3221\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3037 - val_loss: 0.3273\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3022 - val_loss: 0.3317\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3022 - val_loss: 0.3183\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3032 - val_loss: 0.3198\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3010 - val_loss: 0.3191\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3008 - val_loss: 0.3161\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2998 - val_loss: 0.3296\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3000 - val_loss: 0.3210\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2977 - val_loss: 0.3213\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2994 - val_loss: 0.3263\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2984 - val_loss: 0.3196\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2969 - val_loss: 0.3213\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2981 - val_loss: 0.3172\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2974 - val_loss: 0.3152\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2961 - val_loss: 0.3341\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2964 - val_loss: 0.3139\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2952 - val_loss: 0.3298\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2967 - val_loss: 0.3285\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.2952 - val_loss: 0.3180\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2946 - val_loss: 0.3237\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2938 - val_loss: 0.3157\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2947 - val_loss: 0.3126\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2920 - val_loss: 0.3285\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2936 - val_loss: 0.3250\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2929 - val_loss: 0.3159\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2918 - val_loss: 0.3141\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2921 - val_loss: 0.3197\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2929 - val_loss: 0.3278\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2919 - val_loss: 0.3275\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.2925 - val_loss: 0.3120\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2900 - val_loss: 0.3310\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2911 - val_loss: 0.3172\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2901 - val_loss: 0.3367\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2913 - val_loss: 0.3115\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2891 - val_loss: 0.3112\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2885 - val_loss: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2880 - val_loss: 0.3175\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2883 - val_loss: 0.3160\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2874 - val_loss: 0.3249\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.2873 - val_loss: 0.3199\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.2886 - val_loss: 0.3170\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 456us/step - loss: 0.2868 - val_loss: 0.3110\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 460us/step - loss: 0.2869 - val_loss: 0.3162\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 443us/step - loss: 0.2875 - val_loss: 0.3293\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 460us/step - loss: 0.2874 - val_loss: 0.3269\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.2865 - val_loss: 0.3155\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2868 - val_loss: 0.3261\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2852 - val_loss: 0.3094\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 454us/step - loss: 0.2863 - val_loss: 0.3129\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2860 - val_loss: 0.3111\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2848 - val_loss: 0.3156\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2864 - val_loss: 0.3222\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2864 - val_loss: 0.3141\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2844 - val_loss: 0.3108\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2850 - val_loss: 0.3114\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2846 - val_loss: 0.3133\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2840 - val_loss: 0.3174\n",
      "121/121 [==============================] - 0s 251us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.9838 - val_loss: 0.6015\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.5196 - val_loss: 0.5019\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4541 - val_loss: 0.4611\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.4274 - val_loss: 0.4393\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4159 - val_loss: 0.4314\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4029 - val_loss: 0.4227\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3988 - val_loss: 0.4141\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3959 - val_loss: 0.4162\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3885 - val_loss: 0.4253\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3892 - val_loss: 0.4056\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3825 - val_loss: 0.4055\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3810 - val_loss: 0.3952\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3781 - val_loss: 0.3963\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3762 - val_loss: 0.4027\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3722 - val_loss: 0.4243\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3693 - val_loss: 0.4042\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3678 - val_loss: 0.3901\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3645 - val_loss: 0.3860\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3634 - val_loss: 0.3813\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3600 - val_loss: 0.3759\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3568 - val_loss: 0.3827\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3574 - val_loss: 0.3754\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3560 - val_loss: 0.3728\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3535 - val_loss: 0.3674\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3520 - val_loss: 0.3729\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3502 - val_loss: 0.3693\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3473 - val_loss: 0.3627\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3471 - val_loss: 0.3764\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3449 - val_loss: 0.3648\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3429 - val_loss: 0.3613\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3423 - val_loss: 0.3635\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3401 - val_loss: 0.3692\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3396 - val_loss: 0.3587\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3382 - val_loss: 0.3600\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3387 - val_loss: 0.3526\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3369 - val_loss: 0.3544\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3353 - val_loss: 0.3505\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3356 - val_loss: 0.3469\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3328 - val_loss: 0.3434\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3337 - val_loss: 0.3497\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3309 - val_loss: 0.3441\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3293 - val_loss: 0.3434\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3297 - val_loss: 0.3453\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3292 - val_loss: 0.3481\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3287 - val_loss: 0.3416\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3270 - val_loss: 0.3489\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3271 - val_loss: 0.3459\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3272 - val_loss: 0.3444\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3262 - val_loss: 0.3386\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3242 - val_loss: 0.3421\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3232 - val_loss: 0.3355\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3246 - val_loss: 0.3345\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3234 - val_loss: 0.3343\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3217 - val_loss: 0.3442\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3226 - val_loss: 0.3336\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3208 - val_loss: 0.3348\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3182 - val_loss: 0.3304\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3195 - val_loss: 0.3337\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3180 - val_loss: 0.3295\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3185 - val_loss: 0.3272\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3180 - val_loss: 0.3286\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3170 - val_loss: 0.3291\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3183 - val_loss: 0.3293\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3163 - val_loss: 0.3315\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3161 - val_loss: 0.3299\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3167 - val_loss: 0.3266\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3151 - val_loss: 0.3256\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3157 - val_loss: 0.3347\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3156 - val_loss: 0.3236\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3137 - val_loss: 0.3302\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3132 - val_loss: 0.3363\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3137 - val_loss: 0.3253\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3129 - val_loss: 0.3264\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3139 - val_loss: 0.3259\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3117 - val_loss: 0.3233\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3131 - val_loss: 0.3228\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3112 - val_loss: 0.3273\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3113 - val_loss: 0.3233\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3109 - val_loss: 0.3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3120 - val_loss: 0.3251\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3104 - val_loss: 0.3226\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3094 - val_loss: 0.3235\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3103 - val_loss: 0.3240\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3086 - val_loss: 0.3263\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3090 - val_loss: 0.3233\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3089 - val_loss: 0.3208\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3087 - val_loss: 0.3180\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3084 - val_loss: 0.3311\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3077 - val_loss: 0.3210\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3064 - val_loss: 0.3246\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3067 - val_loss: 0.3241\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3070 - val_loss: 0.3239\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3062 - val_loss: 0.3202\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3071 - val_loss: 0.3222\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3059 - val_loss: 0.3261\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3067 - val_loss: 0.3193\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3065 - val_loss: 0.3189\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3045 - val_loss: 0.3224\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3058 - val_loss: 0.3188\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3039 - val_loss: 0.3213\n",
      "121/121 [==============================] - 0s 255us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 1.0319 - val_loss: 0.8291\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.7304 - val_loss: 0.6076\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.5483 - val_loss: 0.4940\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4545 - val_loss: 0.4455\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4183 - val_loss: 0.4172\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3972 - val_loss: 0.4087\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3862 - val_loss: 0.3974\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3788 - val_loss: 0.3941\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3736 - val_loss: 0.3859\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3687 - val_loss: 0.3823\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3664 - val_loss: 0.3798\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3623 - val_loss: 0.3797\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3597 - val_loss: 0.3748\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3563 - val_loss: 0.3711\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3541 - val_loss: 0.3726\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3522 - val_loss: 0.3691\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3504 - val_loss: 0.3675\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3481 - val_loss: 0.3691\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3474 - val_loss: 0.3651\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3441 - val_loss: 0.3663\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3438 - val_loss: 0.3640\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3432 - val_loss: 0.3596\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3412 - val_loss: 0.3626\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3412 - val_loss: 0.3574\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3393 - val_loss: 0.3594\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3377 - val_loss: 0.3548\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3380 - val_loss: 0.3618\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3355 - val_loss: 0.3556\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3372 - val_loss: 0.3553\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3354 - val_loss: 0.3548\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3336 - val_loss: 0.3563\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3334 - val_loss: 0.3548\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3335 - val_loss: 0.3555\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3326 - val_loss: 0.3524\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3321 - val_loss: 0.3562\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3322 - val_loss: 0.3577\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3334 - val_loss: 0.3574\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3316 - val_loss: 0.3594\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3304 - val_loss: 0.3532\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 443us/step - loss: 0.3297 - val_loss: 0.3556\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3301 - val_loss: 0.3551\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3296 - val_loss: 0.3536\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3290 - val_loss: 0.3475\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3274 - val_loss: 0.3466\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3283 - val_loss: 0.3556\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3258 - val_loss: 0.3497\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3268 - val_loss: 0.3454\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3251 - val_loss: 0.3600\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3251 - val_loss: 0.3479\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3244 - val_loss: 0.3446\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3255 - val_loss: 0.3471\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3248 - val_loss: 0.3490\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3247 - val_loss: 0.3421\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3244 - val_loss: 0.3477\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3219 - val_loss: 0.3513\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3226 - val_loss: 0.3458\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3217 - val_loss: 0.3428\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3217 - val_loss: 0.3477\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3202 - val_loss: 0.3447\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3204 - val_loss: 0.3412\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3205 - val_loss: 0.3459\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3190 - val_loss: 0.3501\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3188 - val_loss: 0.3533\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3198 - val_loss: 0.3573\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3194 - val_loss: 0.3559\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3188 - val_loss: 0.3424\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3190 - val_loss: 0.3468\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3193 - val_loss: 0.3473\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3188 - val_loss: 0.3378\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3174 - val_loss: 0.3458\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3171 - val_loss: 0.3401\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3181 - val_loss: 0.3448\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3186 - val_loss: 0.3509\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3173 - val_loss: 0.3536\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3198 - val_loss: 0.3409\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3169 - val_loss: 0.3481\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3199 - val_loss: 0.3441\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3198 - val_loss: 0.3515\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3190 - val_loss: 0.3489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3181 - val_loss: 0.3467\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3158 - val_loss: 0.3484\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3167 - val_loss: 0.3421\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3137 - val_loss: 0.3411\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3155 - val_loss: 0.3368\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3158 - val_loss: 0.3373\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3148 - val_loss: 0.3373\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3141 - val_loss: 0.3366\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3138 - val_loss: 0.3421\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3152 - val_loss: 0.3372\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3130 - val_loss: 0.3449\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3131 - val_loss: 0.3347\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3133 - val_loss: 0.3342\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3141 - val_loss: 0.3353\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3133 - val_loss: 0.3393\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3134 - val_loss: 0.3353\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3130 - val_loss: 0.3375\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3119 - val_loss: 0.3343\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3135 - val_loss: 0.3387\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3126 - val_loss: 0.3369\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3120 - val_loss: 0.3484\n",
      "121/121 [==============================] - 0s 251us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.8142 - val_loss: 0.5290\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4693 - val_loss: 0.4648\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.4312 - val_loss: 0.4448\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4199 - val_loss: 0.4317\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.4114 - val_loss: 0.4231\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4052 - val_loss: 0.4150\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3993 - val_loss: 0.4138\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3989 - val_loss: 0.4104\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3937 - val_loss: 0.4150\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3904 - val_loss: 0.4120\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3890 - val_loss: 0.4034\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3859 - val_loss: 0.4003\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3845 - val_loss: 0.4103\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3823 - val_loss: 0.4035\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3821 - val_loss: 0.3967\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3783 - val_loss: 0.4037\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3768 - val_loss: 0.3922\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3776 - val_loss: 0.3963\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3759 - val_loss: 0.3875\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3733 - val_loss: 0.3913\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3725 - val_loss: 0.3959\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3703 - val_loss: 0.3860\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3702 - val_loss: 0.3874\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3678 - val_loss: 0.3871\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3664 - val_loss: 0.3875\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3631 - val_loss: 0.4143\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3628 - val_loss: 0.3830\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3610 - val_loss: 0.3822\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3596 - val_loss: 0.3871\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3593 - val_loss: 0.3815\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3567 - val_loss: 0.3714\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3549 - val_loss: 0.3724\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3535 - val_loss: 0.3773\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3528 - val_loss: 0.3715\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3515 - val_loss: 0.3732\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3502 - val_loss: 0.3662\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3503 - val_loss: 0.3662\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3477 - val_loss: 0.3706\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3471 - val_loss: 0.3881\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3446 - val_loss: 0.3672\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3451 - val_loss: 0.3648\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 444us/step - loss: 0.3433 - val_loss: 0.3645\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3439 - val_loss: 0.3662\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3414 - val_loss: 0.3665\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3419 - val_loss: 0.3601\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3392 - val_loss: 0.3632\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3381 - val_loss: 0.3575\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3383 - val_loss: 0.3658\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3387 - val_loss: 0.3604\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3368 - val_loss: 0.3623\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3353 - val_loss: 0.3793\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3362 - val_loss: 0.3578\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3355 - val_loss: 0.3622\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3338 - val_loss: 0.3555\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3348 - val_loss: 0.3604\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3332 - val_loss: 0.3727\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3349 - val_loss: 0.3561\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3323 - val_loss: 0.3534\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3315 - val_loss: 0.3593\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3306 - val_loss: 0.3526\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3315 - val_loss: 0.3756\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3303 - val_loss: 0.3604\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3305 - val_loss: 0.3571\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3289 - val_loss: 0.3515\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3283 - val_loss: 0.3568\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3275 - val_loss: 0.3577\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3274 - val_loss: 0.3547\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3257 - val_loss: 0.3599\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 0.3270 - val_loss: 0.3492\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3242 - val_loss: 0.3524\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3251 - val_loss: 0.3494\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3249 - val_loss: 0.3519\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3247 - val_loss: 0.3538\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3236 - val_loss: 0.3514\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3240 - val_loss: 0.3438\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3235 - val_loss: 0.3435\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3232 - val_loss: 0.3459\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3203 - val_loss: 0.3404\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3206 - val_loss: 0.3531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3213 - val_loss: 0.3519\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3204 - val_loss: 0.3376\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3206 - val_loss: 0.3396\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3203 - val_loss: 0.3379\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3183 - val_loss: 0.3421\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3182 - val_loss: 0.3405\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3197 - val_loss: 0.3384\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3188 - val_loss: 0.3457\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.3177 - val_loss: 0.3335\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.3184 - val_loss: 0.3344\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3163 - val_loss: 0.3341\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3154 - val_loss: 0.3450\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 442us/step - loss: 0.3168 - val_loss: 0.3438\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3162 - val_loss: 0.3513\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 448us/step - loss: 0.3162 - val_loss: 0.3311\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3163 - val_loss: 0.3361\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3158 - val_loss: 0.3324\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3145 - val_loss: 0.3339\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3154 - val_loss: 0.3400\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3156 - val_loss: 0.3323\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3155 - val_loss: 0.3335\n",
      "121/121 [==============================] - 0s 234us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 1.1512 - val_loss: 0.7347\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.7271 - val_loss: 0.5590\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4950 - val_loss: 0.4838\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4528 - val_loss: 0.4635\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.4342 - val_loss: 0.4462\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.4240 - val_loss: 0.4380\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.4168 - val_loss: 0.4355\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4095 - val_loss: 0.4290\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.4061 - val_loss: 0.4189\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.4032 - val_loss: 0.4182\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4014 - val_loss: 0.4133\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3974 - val_loss: 0.4128\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3975 - val_loss: 0.4092\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3945 - val_loss: 0.4114\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3941 - val_loss: 0.4121\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3923 - val_loss: 0.4038\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3908 - val_loss: 0.4030\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3893 - val_loss: 0.4040\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3881 - val_loss: 0.4030\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3869 - val_loss: 0.4049\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3854 - val_loss: 0.3997\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3835 - val_loss: 0.3999\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3847 - val_loss: 0.3960\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3819 - val_loss: 0.4042\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3811 - val_loss: 0.4010\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3805 - val_loss: 0.3969\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3787 - val_loss: 0.3978\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3788 - val_loss: 0.3981\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3772 - val_loss: 0.3985\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3768 - val_loss: 0.3903\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3749 - val_loss: 0.3891\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3743 - val_loss: 0.3926\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3739 - val_loss: 0.3921\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3728 - val_loss: 0.3875\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3725 - val_loss: 0.3858\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3733 - val_loss: 0.3946\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3698 - val_loss: 0.3879\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3681 - val_loss: 0.3931\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3702 - val_loss: 0.3853\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3663 - val_loss: 0.3836\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3644 - val_loss: 0.3850\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3643 - val_loss: 0.3784\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3634 - val_loss: 0.3816\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3642 - val_loss: 0.3818\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3613 - val_loss: 0.3755\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3599 - val_loss: 0.3774\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3579 - val_loss: 0.3797\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3579 - val_loss: 0.3762\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3567 - val_loss: 0.3700\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3552 - val_loss: 0.3712\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3553 - val_loss: 0.3690\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3539 - val_loss: 0.3985\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3541 - val_loss: 0.3674\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3538 - val_loss: 0.3694\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3524 - val_loss: 0.3808\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3503 - val_loss: 0.3658\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3507 - val_loss: 0.3747\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3498 - val_loss: 0.3633\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3479 - val_loss: 0.3631\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3496 - val_loss: 0.3651\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3464 - val_loss: 0.3616\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3479 - val_loss: 0.3604\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3437 - val_loss: 0.3589\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3463 - val_loss: 0.3584\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3462 - val_loss: 0.3626\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3432 - val_loss: 0.3607\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3431 - val_loss: 0.3624\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3422 - val_loss: 0.3662\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3420 - val_loss: 0.3558\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3418 - val_loss: 0.3609\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3420 - val_loss: 0.3570\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3408 - val_loss: 0.3560\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3392 - val_loss: 0.3555\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3394 - val_loss: 0.3560\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3409 - val_loss: 0.3609\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3396 - val_loss: 0.3510\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3388 - val_loss: 0.3557\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3369 - val_loss: 0.3553\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3380 - val_loss: 0.3547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3365 - val_loss: 0.3546\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3368 - val_loss: 0.3515\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3360 - val_loss: 0.3534\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3361 - val_loss: 0.3471\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3356 - val_loss: 0.3448\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.3340 - val_loss: 0.3475\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.3349 - val_loss: 0.3490\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3330 - val_loss: 0.3500\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3373 - val_loss: 0.3472\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3347 - val_loss: 0.3545\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3348 - val_loss: 0.3547\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3350 - val_loss: 0.3547\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3332 - val_loss: 0.3443\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3358 - val_loss: 0.3426\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3312 - val_loss: 0.3479\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3317 - val_loss: 0.3473\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3304 - val_loss: 0.3442\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3369 - val_loss: 0.3477\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3308 - val_loss: 0.3462\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3320 - val_loss: 0.3484\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3304 - val_loss: 0.3435\n",
      "121/121 [==============================] - 0s 229us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 1.0446 - val_loss: 1.4788\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 1.4430 - val_loss: 0.5277\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.4728 - val_loss: 0.4298\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.4197 - val_loss: 0.4248\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.4072 - val_loss: 0.4060\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3856 - val_loss: 0.3917\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3742 - val_loss: 0.3926\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3647 - val_loss: 0.3773\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3612 - val_loss: 0.3662\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3542 - val_loss: 0.3688\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3521 - val_loss: 0.3661\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3481 - val_loss: 0.3604\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3438 - val_loss: 0.3574\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3422 - val_loss: 0.3585\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3393 - val_loss: 0.3602\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3392 - val_loss: 0.3514\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3362 - val_loss: 0.3503\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3353 - val_loss: 0.3552\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3343 - val_loss: 0.3565\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3339 - val_loss: 0.3508\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3315 - val_loss: 0.3516\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3294 - val_loss: 0.3447\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3289 - val_loss: 0.3502\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3277 - val_loss: 0.3470\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3254 - val_loss: 0.3438\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3339 - val_loss: 0.3442\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3265 - val_loss: 0.3404\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3249 - val_loss: 0.3422\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3231 - val_loss: 0.3375\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3222 - val_loss: 0.3370\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3214 - val_loss: 0.3398\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3214 - val_loss: 0.3365\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3196 - val_loss: 0.3363\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3186 - val_loss: 0.3393\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3190 - val_loss: 0.3373\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3187 - val_loss: 0.3368\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3175 - val_loss: 0.3369\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3177 - val_loss: 0.3377\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3167 - val_loss: 0.3433\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3208 - val_loss: 0.3329\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3150 - val_loss: 0.3378\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3160 - val_loss: 0.3339\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3147 - val_loss: 0.3356\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3174 - val_loss: 0.3341\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3152 - val_loss: 0.3351\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3151 - val_loss: 0.3320\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3153 - val_loss: 0.3381\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3164 - val_loss: 0.3310\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3132 - val_loss: 0.3326\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3145 - val_loss: 0.3311\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3138 - val_loss: 0.3319\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3123 - val_loss: 0.3649\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3145 - val_loss: 0.3304\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3118 - val_loss: 0.3315\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3128 - val_loss: 0.3290\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3133 - val_loss: 0.3271\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3114 - val_loss: 0.3277\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3116 - val_loss: 0.3576\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3128 - val_loss: 0.3283\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3100 - val_loss: 0.3257\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3113 - val_loss: 0.3273\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3104 - val_loss: 0.3335\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3097 - val_loss: 0.3329\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3117 - val_loss: 0.3319\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3103 - val_loss: 0.3326\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3100 - val_loss: 0.3257\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3099 - val_loss: 0.3308\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3117 - val_loss: 0.3412\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3091 - val_loss: 0.3287\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3090 - val_loss: 0.3343\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3108 - val_loss: 0.3484\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3086 - val_loss: 0.3341\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3090 - val_loss: 0.3312\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3104 - val_loss: 0.3246\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3093 - val_loss: 0.3331\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3095 - val_loss: 0.3315\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3100 - val_loss: 0.3248\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3097 - val_loss: 0.3298\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3108 - val_loss: 0.3303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3235 - val_loss: 0.3313\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3095 - val_loss: 0.3235\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3082 - val_loss: 0.3266\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3089 - val_loss: 0.3260\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3077 - val_loss: 0.3257\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3073 - val_loss: 0.3235\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3067 - val_loss: 0.3295\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3078 - val_loss: 0.3248\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3072 - val_loss: 0.3246\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3081 - val_loss: 0.3221\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3064 - val_loss: 0.3239\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3064 - val_loss: 0.3239\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3057 - val_loss: 0.3284\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3056 - val_loss: 0.3234\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3057 - val_loss: 0.3255\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3100 - val_loss: 0.3212\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3064 - val_loss: 0.3198\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3056 - val_loss: 0.3348\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3059 - val_loss: 0.3338\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3071 - val_loss: 0.3231\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3076 - val_loss: 0.3253\n",
      "121/121 [==============================] - 0s 251us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 1.0021 - val_loss: 0.5997\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.5452 - val_loss: 0.5303\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4872 - val_loss: 0.4857\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.4561 - val_loss: 0.4812\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4433 - val_loss: 0.4555\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4285 - val_loss: 0.4385\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.4195 - val_loss: 0.4315\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.4103 - val_loss: 0.4229\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4034 - val_loss: 0.4154\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3992 - val_loss: 0.4218\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3961 - val_loss: 0.4059\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3933 - val_loss: 0.4091\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3875 - val_loss: 0.4038\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3819 - val_loss: 0.3976\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3795 - val_loss: 0.3937\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3803 - val_loss: 0.4045\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3746 - val_loss: 0.4125\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3757 - val_loss: 0.3929\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.4336 - val_loss: 0.3922\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.3729 - val_loss: 0.3854\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3713 - val_loss: 0.3846\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3687 - val_loss: 0.3824\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3682 - val_loss: 0.3829\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3668 - val_loss: 0.3755\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3654 - val_loss: 0.3759\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3636 - val_loss: 0.3771\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3618 - val_loss: 0.3794\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3625 - val_loss: 0.3801\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3609 - val_loss: 0.3731\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3598 - val_loss: 0.3692\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 403us/step - loss: 0.3592 - val_loss: 0.3703\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3583 - val_loss: 0.3734\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3574 - val_loss: 0.3658\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3562 - val_loss: 0.3725\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3560 - val_loss: 0.3689\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3557 - val_loss: 0.3692\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3535 - val_loss: 0.3690\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3521 - val_loss: 0.3677\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3551 - val_loss: 0.3691\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3547 - val_loss: 0.3644\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3535 - val_loss: 0.3733\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3506 - val_loss: 0.3597\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3508 - val_loss: 0.3577\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.3525 - val_loss: 0.3626\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3503 - val_loss: 0.3656\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3489 - val_loss: 0.3841\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3494 - val_loss: 0.3600\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3470 - val_loss: 0.3586\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3468 - val_loss: 0.3727\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3472 - val_loss: 0.3587\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3473 - val_loss: 0.3573\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3450 - val_loss: 0.3632\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3476 - val_loss: 0.3569\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3462 - val_loss: 0.3720\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3488 - val_loss: 0.3685\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3447 - val_loss: 0.3545\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3445 - val_loss: 0.3512\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 450us/step - loss: 0.3426 - val_loss: 0.3497\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3391 - val_loss: 0.3494\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3429 - val_loss: 0.3502\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3396 - val_loss: 0.3512\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3414 - val_loss: 0.3512\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3391 - val_loss: 0.3535\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3395 - val_loss: 0.3480\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3370 - val_loss: 0.3445\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3345 - val_loss: 0.3522\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3395 - val_loss: 0.3726\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3361 - val_loss: 0.3535\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3741 - val_loss: 0.3591\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3417 - val_loss: 0.3621\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3348 - val_loss: 0.3574\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3351 - val_loss: 0.3769\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3350 - val_loss: 0.3885\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3373 - val_loss: 0.4513\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3427 - val_loss: 0.4929\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3504 - val_loss: 0.6670\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3794 - val_loss: 0.7661\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3875 - val_loss: 0.7335\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3780 - val_loss: 0.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3435 - val_loss: 0.3589\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3348 - val_loss: 0.3459\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3318 - val_loss: 0.3498\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3297 - val_loss: 0.3414\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3371 - val_loss: 0.3363\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.3306 - val_loss: 0.3603\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3285 - val_loss: 0.3386\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 404us/step - loss: 0.3288 - val_loss: 0.3339\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3295 - val_loss: 0.3404\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3300 - val_loss: 0.3328\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3262 - val_loss: 0.3517\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3282 - val_loss: 0.3366\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3241 - val_loss: 0.3354\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3254 - val_loss: 0.3351\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3235 - val_loss: 0.3338\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3245 - val_loss: 0.4379\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3658 - val_loss: 0.3352\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3242 - val_loss: 0.3300\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3261 - val_loss: 0.3345\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3242 - val_loss: 0.3309\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3210 - val_loss: 0.3343\n",
      "121/121 [==============================] - 0s 258us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 1.6125 - val_loss: 0.6785\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5891 - val_loss: 0.5923\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.6526 - val_loss: 0.5724\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.6067 - val_loss: 0.5561\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5278 - val_loss: 0.5662\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.6212 - val_loss: 0.5489\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 379us/step - loss: 0.5361 - val_loss: 0.5820\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.5285 - val_loss: 0.6232\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5565 - val_loss: 0.5424\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5475 - val_loss: 1.3586\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 2.8849 - val_loss: 0.8561\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 1.6229 - val_loss: 0.5955\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.5350 - val_loss: 0.6269\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.7500 - val_loss: 0.6073\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.9380 - val_loss: 0.5674\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 383us/step - loss: 0.5365 - val_loss: 0.6343\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.6440 - val_loss: 0.5800\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.5486 - val_loss: 1.3926\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.6081 - val_loss: 0.6103\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.8349 - val_loss: 0.5413\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5304 - val_loss: 0.6005\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.5272 - val_loss: 0.5563\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.6066 - val_loss: 0.5385\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.6302 - val_loss: 0.5487\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 382us/step - loss: 0.5223 - val_loss: 0.5577\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.7774 - val_loss: 0.5517\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5460 - val_loss: 0.7992\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 1.1102 - val_loss: 0.6366\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.8626 - val_loss: 0.5478\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.5301 - val_loss: 0.5418\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5430 - val_loss: 1.0543\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5732 - val_loss: 0.5396\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5581 - val_loss: 0.5517\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5239 - val_loss: 0.5433\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5390 - val_loss: 0.5544\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 381us/step - loss: 0.5287 - val_loss: 0.6239\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.7561 - val_loss: 0.5931\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.5546 - val_loss: 0.6332\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 0.5335 - val_loss: 0.5424\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.5355 - val_loss: 0.6785\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.7960 - val_loss: 0.6025\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5506 - val_loss: 0.7551\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.5436 - val_loss: 0.5447\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5327 - val_loss: 0.6867\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.8851 - val_loss: 0.6032\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.9184 - val_loss: 0.5392\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 0.5253 - val_loss: 0.5613\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 0.5434 - val_loss: 0.6555\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.5375 - val_loss: 0.5367\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5211 - val_loss: 0.5687\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5363 - val_loss: 0.5513\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5281 - val_loss: 0.5411\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5278 - val_loss: 0.7175\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.8710 - val_loss: 0.6005\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5389 - val_loss: 0.5897\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.6851 - val_loss: 0.6183\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.5583 - val_loss: 0.6759\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.6122 - val_loss: 0.5555\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5316 - val_loss: 0.8945\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 1.1096 - val_loss: 0.6087\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.9382 - val_loss: 0.6071\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5657 - val_loss: 0.5616\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5329 - val_loss: 0.5596\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5353 - val_loss: 0.6834\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5862 - val_loss: 0.5411\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5360 - val_loss: 0.7134\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.7059 - val_loss: 0.5602\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.6326 - val_loss: 0.5478\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5261 - val_loss: 0.5433\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5217 - val_loss: 0.5515\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5882 - val_loss: 0.5398\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5361 - val_loss: 0.5807\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5274 - val_loss: 0.5435\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5306 - val_loss: 0.6405\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.7977 - val_loss: 0.5944\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5446 - val_loss: 0.7760\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.7521 - val_loss: 0.5745\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5315 - val_loss: 0.6500\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.5372 - val_loss: 0.6336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5429 - val_loss: 0.5575\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5380 - val_loss: 1.0395\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5739 - val_loss: 0.5723\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.5323 - val_loss: 0.5596\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5384 - val_loss: 0.7129\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.7251 - val_loss: 0.5697\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.6409 - val_loss: 0.5366\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5621 - val_loss: 0.5405\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5581 - val_loss: 0.5417\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5218 - val_loss: 0.5613\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.6746 - val_loss: 0.5441\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5206 - val_loss: 0.5588\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5336 - val_loss: 0.8713\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5559 - val_loss: 0.5474\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5459 - val_loss: 1.3172\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.6076 - val_loss: 0.5484\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5254 - val_loss: 0.5491\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5429 - val_loss: 0.5493\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5243 - val_loss: 0.5667\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5234 - val_loss: 0.5560\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5961 - val_loss: 0.5381\n",
      "121/121 [==============================] - 0s 238us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 533us/step - loss: 1.4434 - val_loss: 2.5001\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 2.4399 - val_loss: 31.7334\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 35.7431 - val_loss: 514.8444\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 278.2601 - val_loss: 8363.1484\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 4221.9302 - val_loss: 137386.5000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 132562.2344 - val_loss: 2220534.2500\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 1094369.1250 - val_loss: 36600268.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 17602846.0000 - val_loss: 598932480.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 321589440.0000 - val_loss: 9694988288.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 4902453760.0000 - val_loss: 158320295936.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 152869257216.0000 - val_loss: 2564410048512.0000\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 1310063198208.0000 - val_loss: 42074569179136.0000\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 47474697830400.0000 - val_loss: 681998460911616.0000\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: 609962799661056.0000 - val_loss: 11111704238751744.0000\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 8588998289129472.0000 - val_loss: 181039554495512576.0000\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 171336141042089984.0000 - val_loss: 2948836584503377920.0000\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 1765020040106606592.0000 - val_loss: 48584867964444999680.0000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 35524820471210049536.0000 - val_loss: 789021366041079447552.0000\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 912159561108829503488.0000 - val_loss: 12904561454971797635072.0000\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 13884680967776345522176.0000 - val_loss: 210931204637073873043456.0000\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 100241922146746171916288.0000 - val_loss: 3487408272141745737695232.0000\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 1885625746869026138619904.0000 - val_loss: 56315792893152802912600064.0000\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 26304757289518622824202240.0000 - val_loss: 924048987748541532351758336.0000\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 969766660391060336694788096.0000 - val_loss: 14870902059749896085156397056.0000\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 16030042330718872025197182976.0000 - val_loss: 242130123349766496784805265408.0000\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 178086843597965092247385931776.0000 - val_loss: 3975978099691212390490756349952.0000\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 5055255389864364792271211593728.0000 - val_loss: 64709292248515576384458296655872.0000\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 61791796403742870255656580939776.0000 - val_loss: 1054359566417049896360424234811392.0000\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 808557364437895457763408557899776.0000 - val_loss: 17258869803461214782054729877815296.0000\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 8839978172882088083970970940866560.0000 - val_loss: inf\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: inf - val_loss: inf            \n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: inf - val_loss: inf             \n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: inf - val_loss: inf              \n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: inf - val_loss: inf               \n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: inf - val_loss: inf\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: inf - val_loss: inf\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: inf - val_loss: inf\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: inf - val_loss: inf\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: inf - val_loss: inf\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: inf - val_loss: inf\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: inf - val_loss: inf\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: inf - val_loss: inf\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: inf - val_loss: inf\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: inf - val_loss: inf\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: inf - val_loss: inf\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: inf - val_loss: inf\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: inf - val_loss: inf\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: inf - val_loss: inf\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: inf - val_loss: inf\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: inf - val_loss: inf\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: inf - val_loss: inf\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: inf - val_loss: inf\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: inf - val_loss: inf\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: inf - val_loss: inf\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: inf - val_loss: inf\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: inf - val_loss: inf\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: inf - val_loss: inf\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: inf - val_loss: inf\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: inf - val_loss: inf\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: inf - val_loss: inf\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 403us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 401us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 398us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 398us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 376us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 241us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/scikeras/wrappers.py\", line 1117, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/scikeras/wrappers.py\", line 1714, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 989, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 101, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 959, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 124, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 173, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 1.1947 - val_loss: 0.6286\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.5510 - val_loss: 0.5976\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5339 - val_loss: 0.5671\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5235 - val_loss: 0.5574\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5215 - val_loss: 0.5662\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5222 - val_loss: 0.5521\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5169 - val_loss: 0.5709\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5271 - val_loss: 0.5448\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5182 - val_loss: 0.5782\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5189 - val_loss: 0.5520\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5196 - val_loss: 0.5574\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5180 - val_loss: 0.5822\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5219 - val_loss: 0.5470\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5192 - val_loss: 0.5625\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5216 - val_loss: 0.5510\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5217 - val_loss: 0.5533\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5214 - val_loss: 0.5772\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5212 - val_loss: 0.5637\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5219 - val_loss: 0.5570\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5229 - val_loss: 0.5458\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5217 - val_loss: 0.5528\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5180 - val_loss: 0.5532\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5239 - val_loss: 0.5412\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5229 - val_loss: 0.5451\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5160 - val_loss: 0.5513\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5206 - val_loss: 0.5434\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5180 - val_loss: 0.5594\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5212 - val_loss: 0.5604\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5205 - val_loss: 0.5582\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5224 - val_loss: 0.5413\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.5204 - val_loss: 0.5422\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5201 - val_loss: 0.5652\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.5197 - val_loss: 0.5542\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 399us/step - loss: 0.5186 - val_loss: 0.5658\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5237 - val_loss: 0.5496\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5211 - val_loss: 0.5420\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5198 - val_loss: 0.5373\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5193 - val_loss: 0.5431\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5181 - val_loss: 0.5583\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5210 - val_loss: 0.5448\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5220 - val_loss: 0.5415\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5172 - val_loss: 0.5859\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5233 - val_loss: 0.5482\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5220 - val_loss: 0.5660\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5210 - val_loss: 0.5397\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5205 - val_loss: 0.5406\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5202 - val_loss: 0.5478\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5205 - val_loss: 0.5602\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5209 - val_loss: 0.5658\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 0.5224 - val_loss: 0.5575\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5212 - val_loss: 0.5406\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 395us/step - loss: 0.5208 - val_loss: 0.5471\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5194 - val_loss: 0.5623\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5189 - val_loss: 0.5742\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5192 - val_loss: 0.5919\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5248 - val_loss: 0.5782\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5223 - val_loss: 0.5451\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5188 - val_loss: 0.5802\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5207 - val_loss: 0.5993\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5222 - val_loss: 0.5675\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5235 - val_loss: 0.5606\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.5202 - val_loss: 0.5731\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.5201 - val_loss: 0.5661\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 398us/step - loss: 0.5176 - val_loss: 0.5738\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5212 - val_loss: 0.5630\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5176 - val_loss: 0.5872\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5233 - val_loss: 0.5494\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5199 - val_loss: 0.5569\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5204 - val_loss: 0.5784\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 397us/step - loss: 0.5241 - val_loss: 0.5615\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5220 - val_loss: 0.5503\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 396us/step - loss: 0.5195 - val_loss: 0.5501\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5264 - val_loss: 0.5473\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 383us/step - loss: 0.5182 - val_loss: 0.5744\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.5197 - val_loss: 0.5671\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5221 - val_loss: 0.5536\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5227 - val_loss: 0.5481\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5231 - val_loss: 0.5480\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5193 - val_loss: 0.5598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5216 - val_loss: 0.5647\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5216 - val_loss: 0.5461\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.5217 - val_loss: 0.5398\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5212 - val_loss: 0.5582\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5231 - val_loss: 0.5428\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5216 - val_loss: 0.5388\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 386us/step - loss: 0.5201 - val_loss: 0.5462\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 394us/step - loss: 0.5188 - val_loss: 0.5433\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 392us/step - loss: 0.5218 - val_loss: 0.5460\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5225 - val_loss: 0.5494\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5184 - val_loss: 0.5670\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.5207 - val_loss: 0.5724\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 385us/step - loss: 0.5183 - val_loss: 0.5677\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5239 - val_loss: 0.5565\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 384us/step - loss: 0.5208 - val_loss: 0.5529\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5201 - val_loss: 0.5710\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 389us/step - loss: 0.5196 - val_loss: 0.5536\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 393us/step - loss: 0.5230 - val_loss: 0.5552\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 390us/step - loss: 0.5209 - val_loss: 0.5642\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.5215 - val_loss: 0.5669\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 379us/step - loss: 0.5225 - val_loss: 0.5506\n",
      "121/121 [==============================] - 0s 217us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 615us/step - loss: 1.0754 - val_loss: 0.6069\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.5266 - val_loss: 0.5103\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4595 - val_loss: 0.4736\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.4318 - val_loss: 0.4453\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4178 - val_loss: 0.4268\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4082 - val_loss: 0.4377\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3984 - val_loss: 0.4210\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3946 - val_loss: 0.4042\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3886 - val_loss: 0.4035\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3863 - val_loss: 0.3899\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3808 - val_loss: 0.3896\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3764 - val_loss: 0.3818\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3719 - val_loss: 0.3884\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3699 - val_loss: 0.3747\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3649 - val_loss: 0.3686\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 458us/step - loss: 0.3627 - val_loss: 0.3712\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3584 - val_loss: 0.3650\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3568 - val_loss: 0.3677\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3539 - val_loss: 0.3647\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3519 - val_loss: 0.3591\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 444us/step - loss: 0.3532 - val_loss: 0.3592\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.3488 - val_loss: 0.3533\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3479 - val_loss: 0.3599\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 449us/step - loss: 0.3462 - val_loss: 0.3532\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.3466 - val_loss: 0.3541\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3456 - val_loss: 0.3572\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3439 - val_loss: 0.3577\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3422 - val_loss: 0.3643\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3418 - val_loss: 0.3484\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3405 - val_loss: 0.3453\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3397 - val_loss: 0.3490\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3386 - val_loss: 0.3459\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3362 - val_loss: 0.3372\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3357 - val_loss: 0.3417\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3343 - val_loss: 0.3390\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3329 - val_loss: 0.3337\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3330 - val_loss: 0.3394\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3313 - val_loss: 0.3383\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3298 - val_loss: 0.3578\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3313 - val_loss: 0.3333\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3294 - val_loss: 0.3318\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3283 - val_loss: 0.3292\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3280 - val_loss: 0.3302\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3258 - val_loss: 0.3355\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3252 - val_loss: 0.3315\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3234 - val_loss: 0.3624\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3255 - val_loss: 0.3246\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3235 - val_loss: 0.3254\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3238 - val_loss: 0.3315\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3206 - val_loss: 0.3309\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3230 - val_loss: 0.3407\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3224 - val_loss: 0.3310\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3211 - val_loss: 0.3246\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3194 - val_loss: 0.3286\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3189 - val_loss: 0.3317\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3201 - val_loss: 0.3222\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3183 - val_loss: 0.3228\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3181 - val_loss: 0.3274\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3181 - val_loss: 0.3283\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3141 - val_loss: 0.3555\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3168 - val_loss: 0.3268\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3154 - val_loss: 0.3242\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3130 - val_loss: 0.3286\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3143 - val_loss: 0.3494\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3140 - val_loss: 0.3220\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3142 - val_loss: 0.3222\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3126 - val_loss: 0.3393\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3135 - val_loss: 0.3197\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3112 - val_loss: 0.3356\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3123 - val_loss: 0.3196\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3107 - val_loss: 0.3523\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3125 - val_loss: 0.3161\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3107 - val_loss: 0.3140\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3108 - val_loss: 0.3201\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3107 - val_loss: 0.3311\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3101 - val_loss: 0.3235\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3113 - val_loss: 0.3180\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3087 - val_loss: 0.3218\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3098 - val_loss: 0.3295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3096 - val_loss: 0.3266\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3073 - val_loss: 0.3197\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3077 - val_loss: 0.3188\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3074 - val_loss: 0.3203\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3090 - val_loss: 0.3249\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3080 - val_loss: 0.3133\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3036 - val_loss: 0.3290\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3049 - val_loss: 0.3378\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3060 - val_loss: 0.3343\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3066 - val_loss: 0.3179\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3044 - val_loss: 0.3163\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3048 - val_loss: 0.3145\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3037 - val_loss: 0.3167\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3037 - val_loss: 0.3182\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3026 - val_loss: 0.3148\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3038 - val_loss: 0.3166\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3028 - val_loss: 0.3304\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3020 - val_loss: 0.3374\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3010 - val_loss: 0.3210\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3031 - val_loss: 0.3151\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3017 - val_loss: 0.3205\n",
      "121/121 [==============================] - 0s 249us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 610us/step - loss: 1.7562 - val_loss: 0.8656\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.6904 - val_loss: 0.6450\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.5735 - val_loss: 0.5682\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4968 - val_loss: 0.4894\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4305 - val_loss: 0.4277\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3972 - val_loss: 0.4099\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3842 - val_loss: 0.3918\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3724 - val_loss: 0.3831\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3648 - val_loss: 0.3830\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3607 - val_loss: 0.3768\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3556 - val_loss: 0.3787\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3525 - val_loss: 0.3752\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3501 - val_loss: 0.3691\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3467 - val_loss: 0.3725\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3458 - val_loss: 0.3769\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3439 - val_loss: 0.3719\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3422 - val_loss: 0.3651\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3417 - val_loss: 0.3649\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3402 - val_loss: 0.3605\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3395 - val_loss: 0.3631\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3387 - val_loss: 0.3640\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3377 - val_loss: 0.3571\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3364 - val_loss: 0.3582\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3361 - val_loss: 0.3684\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3355 - val_loss: 0.3600\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3362 - val_loss: 0.3558\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3343 - val_loss: 0.3580\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3341 - val_loss: 0.3573\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3336 - val_loss: 0.3569\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3318 - val_loss: 0.3664\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3322 - val_loss: 0.3539\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3320 - val_loss: 0.3531\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3313 - val_loss: 0.3642\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3302 - val_loss: 0.3523\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3313 - val_loss: 0.3564\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3306 - val_loss: 0.3562\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3300 - val_loss: 0.3574\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3289 - val_loss: 0.3544\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3288 - val_loss: 0.3592\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3265 - val_loss: 0.3665\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3282 - val_loss: 0.3538\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3275 - val_loss: 0.3526\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3271 - val_loss: 0.3525\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3276 - val_loss: 0.3494\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3276 - val_loss: 0.3523\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3253 - val_loss: 0.3636\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3255 - val_loss: 0.3559\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3264 - val_loss: 0.3502\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3240 - val_loss: 0.3597\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3256 - val_loss: 0.3492\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3243 - val_loss: 0.3532\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3241 - val_loss: 0.3519\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3257 - val_loss: 0.3504\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3245 - val_loss: 0.3502\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3243 - val_loss: 0.3489\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3231 - val_loss: 0.3534\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3224 - val_loss: 0.3513\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3219 - val_loss: 0.3494\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3220 - val_loss: 0.3486\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3213 - val_loss: 0.3498\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3216 - val_loss: 0.3469\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3218 - val_loss: 0.3554\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3211 - val_loss: 0.3488\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3204 - val_loss: 0.3452\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3215 - val_loss: 0.3594\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3191 - val_loss: 0.3562\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3190 - val_loss: 0.3457\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3196 - val_loss: 0.3442\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3199 - val_loss: 0.3599\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3190 - val_loss: 0.3472\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3188 - val_loss: 0.3493\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3175 - val_loss: 0.3501\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3176 - val_loss: 0.3445\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3169 - val_loss: 0.3497\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3173 - val_loss: 0.3444\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 458us/step - loss: 0.3170 - val_loss: 0.3586\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.3156 - val_loss: 0.3483\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3156 - val_loss: 0.3499\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3156 - val_loss: 0.3471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3162 - val_loss: 0.3453\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3147 - val_loss: 0.3465\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3146 - val_loss: 0.3460\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3144 - val_loss: 0.3444\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3153 - val_loss: 0.3452\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3135 - val_loss: 0.3592\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3145 - val_loss: 0.3487\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3134 - val_loss: 0.3429\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3128 - val_loss: 0.3435\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3118 - val_loss: 0.3433\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3128 - val_loss: 0.3458\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3122 - val_loss: 0.3456\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3117 - val_loss: 0.3444\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3115 - val_loss: 0.3460\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3110 - val_loss: 0.3477\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3116 - val_loss: 0.3462\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3112 - val_loss: 0.3423\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3111 - val_loss: 0.3507\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3099 - val_loss: 0.3496\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3093 - val_loss: 0.3444\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3096 - val_loss: 0.3475\n",
      "121/121 [==============================] - 0s 260us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.8893 - val_loss: 0.6037\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.5577 - val_loss: 0.5264\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.5006 - val_loss: 0.4841\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4676 - val_loss: 0.4565\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4425 - val_loss: 0.4427\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4252 - val_loss: 0.4264\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4106 - val_loss: 0.4135\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4004 - val_loss: 0.4065\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3933 - val_loss: 0.4019\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3856 - val_loss: 0.3950\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3822 - val_loss: 0.3879\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3776 - val_loss: 0.3889\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3739 - val_loss: 0.3809\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3691 - val_loss: 0.3790\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3663 - val_loss: 0.3789\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3619 - val_loss: 0.3888\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3597 - val_loss: 0.3806\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3578 - val_loss: 0.3694\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3559 - val_loss: 0.3693\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3538 - val_loss: 0.3661\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3510 - val_loss: 0.3707\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3494 - val_loss: 0.3595\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3474 - val_loss: 0.3604\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3453 - val_loss: 0.3627\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3424 - val_loss: 0.3533\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3414 - val_loss: 0.3503\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3396 - val_loss: 0.3485\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3383 - val_loss: 0.3513\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3358 - val_loss: 0.3467\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3366 - val_loss: 0.3642\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3343 - val_loss: 0.3403\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3317 - val_loss: 0.3577\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3321 - val_loss: 0.3433\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3302 - val_loss: 0.3401\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3286 - val_loss: 0.3415\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3285 - val_loss: 0.3374\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3274 - val_loss: 0.3357\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3261 - val_loss: 0.3477\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3258 - val_loss: 0.3439\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3252 - val_loss: 0.3411\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3228 - val_loss: 0.3372\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3231 - val_loss: 0.3461\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3215 - val_loss: 0.3416\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3215 - val_loss: 0.3414\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3198 - val_loss: 0.3318\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3196 - val_loss: 0.3361\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 460us/step - loss: 0.3191 - val_loss: 0.3340\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3190 - val_loss: 0.3340\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3168 - val_loss: 0.3286\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3186 - val_loss: 0.3241\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3161 - val_loss: 0.3209\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3160 - val_loss: 0.3429\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3141 - val_loss: 0.3249\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3139 - val_loss: 0.3294\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3137 - val_loss: 0.3208\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3135 - val_loss: 0.3349\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3148 - val_loss: 0.3342\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3110 - val_loss: 0.3223\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3158 - val_loss: 0.3223\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3120 - val_loss: 0.3200\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3104 - val_loss: 0.3220\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3138 - val_loss: 0.3205\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3110 - val_loss: 0.3229\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3095 - val_loss: 0.3207\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3108 - val_loss: 0.3223\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3108 - val_loss: 0.3275\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3089 - val_loss: 0.3189\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3123 - val_loss: 0.3254\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3092 - val_loss: 0.3246\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3087 - val_loss: 0.3268\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3078 - val_loss: 0.3192\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3091 - val_loss: 0.3149\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3084 - val_loss: 0.3215\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3067 - val_loss: 0.3157\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3075 - val_loss: 0.3243\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3072 - val_loss: 0.3237\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3066 - val_loss: 0.3227\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3049 - val_loss: 0.3195\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3088 - val_loss: 0.3248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3065 - val_loss: 0.3149\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3083 - val_loss: 0.3289\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3060 - val_loss: 0.3198\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3076 - val_loss: 0.3191\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3070 - val_loss: 0.3192\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3038 - val_loss: 0.3180\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3039 - val_loss: 0.3140\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3046 - val_loss: 0.3168\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3071 - val_loss: 0.3139\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3042 - val_loss: 0.3234\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3046 - val_loss: 0.3204\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3037 - val_loss: 0.3144\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3023 - val_loss: 0.3151\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3027 - val_loss: 0.3332\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3042 - val_loss: 0.3189\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3031 - val_loss: 0.3129\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3040 - val_loss: 0.3152\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3015 - val_loss: 0.3137\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3013 - val_loss: 0.3202\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3038 - val_loss: 0.3142\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3034 - val_loss: 0.3172\n",
      "121/121 [==============================] - 0s 251us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.9694 - val_loss: 1.5010\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 1.4563 - val_loss: 0.5530\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.5008 - val_loss: 0.4439\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4302 - val_loss: 0.4325\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.4080 - val_loss: 0.4203\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3994 - val_loss: 0.4014\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3901 - val_loss: 0.3918\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3865 - val_loss: 0.3955\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3838 - val_loss: 0.3886\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3806 - val_loss: 0.3917\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3789 - val_loss: 0.3818\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3775 - val_loss: 0.3823\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3769 - val_loss: 0.3844\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3744 - val_loss: 0.3797\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.3730 - val_loss: 0.3806\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3731 - val_loss: 0.3780\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3726 - val_loss: 0.3774\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3680 - val_loss: 0.3750\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3701 - val_loss: 0.3787\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3675 - val_loss: 0.3800\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3689 - val_loss: 0.3769\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3665 - val_loss: 0.3992\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3666 - val_loss: 0.3770\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3634 - val_loss: 0.3741\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 458us/step - loss: 0.3646 - val_loss: 0.3685\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 450us/step - loss: 0.3626 - val_loss: 0.3804\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3630 - val_loss: 0.3751\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.3628 - val_loss: 0.3729\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3610 - val_loss: 0.3708\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3601 - val_loss: 0.3661\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3617 - val_loss: 0.3666\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3608 - val_loss: 0.3708\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3592 - val_loss: 0.3808\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3593 - val_loss: 0.3732\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3580 - val_loss: 0.3687\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3567 - val_loss: 0.3762\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3575 - val_loss: 0.3689\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3579 - val_loss: 0.3668\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3564 - val_loss: 0.3692\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3566 - val_loss: 0.3771\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3542 - val_loss: 0.3653\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3552 - val_loss: 0.3701\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3569 - val_loss: 0.3639\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3550 - val_loss: 0.3656\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3530 - val_loss: 0.3667\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3532 - val_loss: 0.3648\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3527 - val_loss: 0.3611\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3573 - val_loss: 0.3708\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3538 - val_loss: 0.3692\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3552 - val_loss: 0.3681\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3534 - val_loss: 0.3630\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3524 - val_loss: 0.3712\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3545 - val_loss: 0.3616\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3522 - val_loss: 0.3668\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3551 - val_loss: 0.3597\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3495 - val_loss: 0.3693\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3565 - val_loss: 0.3664\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3604 - val_loss: 0.3582\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3516 - val_loss: 0.3610\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3513 - val_loss: 0.3698\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3507 - val_loss: 0.3650\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3499 - val_loss: 0.3715\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3554 - val_loss: 0.3678\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3484 - val_loss: 0.3880\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3559 - val_loss: 0.3637\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3481 - val_loss: 0.3734\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3500 - val_loss: 0.3613\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3521 - val_loss: 0.3597\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3490 - val_loss: 0.3861\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3470 - val_loss: 0.3618\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3478 - val_loss: 0.3659\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3466 - val_loss: 0.3549\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3454 - val_loss: 0.3568\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3494 - val_loss: 0.3549\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3489 - val_loss: 0.3556\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3514 - val_loss: 0.3711\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3458 - val_loss: 0.3591\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3450 - val_loss: 0.3597\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3446 - val_loss: 0.3551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3466 - val_loss: 0.3673\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3462 - val_loss: 0.3644\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3511 - val_loss: 0.4338\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3487 - val_loss: 0.3664\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3473 - val_loss: 0.3791\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3461 - val_loss: 0.3552\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3440 - val_loss: 0.3688\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3432 - val_loss: 0.3582\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3461 - val_loss: 0.3596\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3457 - val_loss: 0.3581\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3468 - val_loss: 0.3603\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3476 - val_loss: 0.3525\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3427 - val_loss: 0.3536\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3505 - val_loss: 0.3554\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3426 - val_loss: 0.3501\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3456 - val_loss: 0.3496\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3433 - val_loss: 0.3587\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.3501 - val_loss: 0.4173\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3478 - val_loss: 0.3525\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3465 - val_loss: 0.3500\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3432 - val_loss: 0.3504\n",
      "121/121 [==============================] - 0s 235us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.9792 - val_loss: 0.8460\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.7727 - val_loss: 0.8031\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.6354 - val_loss: 0.4995\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4633 - val_loss: 0.4672\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.4361 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4240 - val_loss: 0.4368\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4145 - val_loss: 0.4281\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.4069 - val_loss: 0.4257\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4017 - val_loss: 0.4199\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3975 - val_loss: 0.4176\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3932 - val_loss: 0.4242\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 408us/step - loss: 0.3904 - val_loss: 0.4085\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3905 - val_loss: 0.4008\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3849 - val_loss: 0.3985\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3806 - val_loss: 0.4022\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3778 - val_loss: 0.3985\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3782 - val_loss: 0.3928\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3742 - val_loss: 0.3894\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3733 - val_loss: 0.3960\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3716 - val_loss: 0.3869\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3702 - val_loss: 0.3852\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3686 - val_loss: 0.3837\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3676 - val_loss: 0.3839\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.3660 - val_loss: 0.3809\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.3649 - val_loss: 0.3794\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3626 - val_loss: 0.3924\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3600 - val_loss: 0.3819\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3606 - val_loss: 0.3852\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3593 - val_loss: 0.3764\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3595 - val_loss: 0.3735\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3561 - val_loss: 0.3741\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3539 - val_loss: 0.3731\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3530 - val_loss: 0.3688\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3531 - val_loss: 0.3696\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3510 - val_loss: 0.3678\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3517 - val_loss: 0.3665\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3502 - val_loss: 0.3686\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3471 - val_loss: 0.3636\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3471 - val_loss: 0.3595\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3453 - val_loss: 0.3726\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3450 - val_loss: 0.3612\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3428 - val_loss: 0.3745\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3447 - val_loss: 0.3628\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3409 - val_loss: 0.3601\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3447 - val_loss: 0.3700\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3417 - val_loss: 0.3579\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3404 - val_loss: 0.3616\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3391 - val_loss: 0.3618\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3381 - val_loss: 0.3674\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3387 - val_loss: 0.3530\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3403 - val_loss: 0.3527\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3369 - val_loss: 0.3631\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3387 - val_loss: 0.3563\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3345 - val_loss: 0.3639\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3347 - val_loss: 0.3656\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3352 - val_loss: 0.3507\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3343 - val_loss: 0.3609\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3357 - val_loss: 0.3556\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3366 - val_loss: 0.3550\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3351 - val_loss: 0.3467\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3315 - val_loss: 0.3533\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3357 - val_loss: 0.3487\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3341 - val_loss: 0.3444\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3309 - val_loss: 0.3438\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3327 - val_loss: 0.3440\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3299 - val_loss: 0.3545\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3300 - val_loss: 0.3653\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3306 - val_loss: 0.3478\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3287 - val_loss: 0.3430\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3318 - val_loss: 0.3510\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3278 - val_loss: 0.3450\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3277 - val_loss: 0.3454\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 402us/step - loss: 0.3275 - val_loss: 0.3426\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 388us/step - loss: 0.3284 - val_loss: 0.3436\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 391us/step - loss: 0.3259 - val_loss: 0.3436\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 387us/step - loss: 0.3263 - val_loss: 0.3492\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.3277 - val_loss: 0.3422\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3241 - val_loss: 0.3431\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3276 - val_loss: 0.3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3239 - val_loss: 0.3545\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3493 - val_loss: 0.3749\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3287 - val_loss: 0.3416\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3263 - val_loss: 0.3473\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3256 - val_loss: 0.3518\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.3255 - val_loss: 0.3389\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3232 - val_loss: 0.3442\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3228 - val_loss: 0.3468\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3219 - val_loss: 0.3462\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3218 - val_loss: 0.3392\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3211 - val_loss: 0.3426\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3206 - val_loss: 0.3438\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3207 - val_loss: 0.3404\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3197 - val_loss: 0.3451\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3202 - val_loss: 0.3429\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3208 - val_loss: 0.3396\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3188 - val_loss: 0.3415\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3194 - val_loss: 0.3393\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3182 - val_loss: 0.3420\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3194 - val_loss: 0.3380\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3183 - val_loss: 0.3391\n",
      "121/121 [==============================] - 0s 248us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.9801 - val_loss: 0.6101\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.5243 - val_loss: 0.5017\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4711 - val_loss: 0.4731\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.4471 - val_loss: 0.4720\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4326 - val_loss: 0.4596\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4234 - val_loss: 0.4422\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4090 - val_loss: 0.4270\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.4030 - val_loss: 0.4163\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3980 - val_loss: 0.4130\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3931 - val_loss: 0.4149\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3894 - val_loss: 0.4048\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3858 - val_loss: 0.4038\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3812 - val_loss: 0.4057\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3815 - val_loss: 0.3984\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3770 - val_loss: 0.3994\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3755 - val_loss: 0.3937\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 407us/step - loss: 0.3708 - val_loss: 0.3949\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3713 - val_loss: 0.3913\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3731 - val_loss: 0.3849\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3658 - val_loss: 0.3833\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3665 - val_loss: 0.3872\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3634 - val_loss: 0.3804\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3639 - val_loss: 0.3786\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3615 - val_loss: 0.3812\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3617 - val_loss: 0.3744\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3594 - val_loss: 0.3777\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3592 - val_loss: 0.3742\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3595 - val_loss: 0.3818\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3564 - val_loss: 0.3857\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3554 - val_loss: 0.3675\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3568 - val_loss: 0.3857\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.3600 - val_loss: 0.3687\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3542 - val_loss: 0.3678\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3547 - val_loss: 0.3681\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3535 - val_loss: 0.3632\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3545 - val_loss: 0.3669\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3540 - val_loss: 0.3663\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3510 - val_loss: 0.3617\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3488 - val_loss: 0.3777\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3512 - val_loss: 0.3710\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3499 - val_loss: 0.3646\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3519 - val_loss: 0.3600\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3479 - val_loss: 0.3661\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3499 - val_loss: 0.3690\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3483 - val_loss: 0.3594\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3454 - val_loss: 0.3664\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3481 - val_loss: 0.3608\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3480 - val_loss: 0.3659\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3466 - val_loss: 0.3750\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3468 - val_loss: 0.3628\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3455 - val_loss: 0.3670\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3459 - val_loss: 0.3633\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3465 - val_loss: 0.3751\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3468 - val_loss: 0.3744\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3487 - val_loss: 0.4089\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3486 - val_loss: 0.3896\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3479 - val_loss: 0.4020\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3478 - val_loss: 0.4049\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3510 - val_loss: 0.4312\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3514 - val_loss: 0.4499\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3539 - val_loss: 0.4929\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3560 - val_loss: 0.4626\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3551 - val_loss: 0.4840\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3545 - val_loss: 0.4692\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3529 - val_loss: 0.4875\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3577 - val_loss: 0.4524\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3538 - val_loss: 0.4379\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3480 - val_loss: 0.3926\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3451 - val_loss: 0.3934\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3438 - val_loss: 0.3613\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3407 - val_loss: 0.3675\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3415 - val_loss: 0.3606\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3407 - val_loss: 0.3553\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3404 - val_loss: 0.3477\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3379 - val_loss: 0.3611\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.3400 - val_loss: 0.3497\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3395 - val_loss: 0.3529\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3373 - val_loss: 0.3532\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3372 - val_loss: 0.3453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3370 - val_loss: 0.3522\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3350 - val_loss: 0.3541\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3363 - val_loss: 0.3487\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3347 - val_loss: 0.3493\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3350 - val_loss: 0.3566\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3340 - val_loss: 0.3485\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3354 - val_loss: 0.3501\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3350 - val_loss: 0.3482\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3349 - val_loss: 0.3544\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3329 - val_loss: 0.3567\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 400us/step - loss: 0.3366 - val_loss: 0.3615\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3342 - val_loss: 0.3695\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3359 - val_loss: 0.4308\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3407 - val_loss: 0.4887\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3497 - val_loss: 0.6680\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3598 - val_loss: 0.7530\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3738 - val_loss: 0.3644\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3351 - val_loss: 0.3434\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3313 - val_loss: 0.3477\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3320 - val_loss: 0.3427\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3316 - val_loss: 0.3430\n",
      "121/121 [==============================] - 0s 258us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 612us/step - loss: 1.4045 - val_loss: 0.7961\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 1.7994 - val_loss: 0.4685\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 442us/step - loss: 0.4333 - val_loss: 0.4123\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4028 - val_loss: 0.3959\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3854 - val_loss: 0.3853\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3767 - val_loss: 0.3776\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3706 - val_loss: 0.3949\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3648 - val_loss: 0.3709\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3631 - val_loss: 0.3644\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3588 - val_loss: 0.3753\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3556 - val_loss: 0.3652\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3549 - val_loss: 0.3653\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3510 - val_loss: 0.3625\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3487 - val_loss: 0.3619\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3466 - val_loss: 0.3544\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3446 - val_loss: 0.3552\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3435 - val_loss: 0.3517\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3394 - val_loss: 0.3590\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3371 - val_loss: 0.3489\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3351 - val_loss: 0.3464\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3341 - val_loss: 0.3530\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3318 - val_loss: 0.3556\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3303 - val_loss: 0.3415\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 443us/step - loss: 0.3291 - val_loss: 0.3476\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 456us/step - loss: 0.3273 - val_loss: 0.3657\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3246 - val_loss: 0.3364\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 456us/step - loss: 0.3252 - val_loss: 0.3348\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3228 - val_loss: 0.3382\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.3205 - val_loss: 0.3361\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3210 - val_loss: 0.3393\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3214 - val_loss: 0.3318\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3171 - val_loss: 0.3502\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3181 - val_loss: 0.3288\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3159 - val_loss: 0.3335\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3181 - val_loss: 0.3287\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3143 - val_loss: 0.3363\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3142 - val_loss: 0.3316\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3132 - val_loss: 0.3291\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3108 - val_loss: 0.3247\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3105 - val_loss: 0.3573\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3108 - val_loss: 0.3274\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3091 - val_loss: 0.3755\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3083 - val_loss: 0.3294\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3076 - val_loss: 0.3240\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3059 - val_loss: 0.3260\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3065 - val_loss: 0.3271\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3066 - val_loss: 0.3321\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3040 - val_loss: 0.3219\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3034 - val_loss: 0.3319\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3045 - val_loss: 0.3225\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3014 - val_loss: 0.3392\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3022 - val_loss: 0.3254\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3017 - val_loss: 0.3252\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3009 - val_loss: 0.3273\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3001 - val_loss: 0.3262\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3002 - val_loss: 0.3207\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2992 - val_loss: 0.3290\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2989 - val_loss: 0.3203\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.2980 - val_loss: 0.3235\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2972 - val_loss: 0.3247\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2972 - val_loss: 0.3207\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2969 - val_loss: 0.3208\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2972 - val_loss: 0.3301\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2946 - val_loss: 0.3221\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2951 - val_loss: 0.3480\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2956 - val_loss: 0.3162\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2944 - val_loss: 0.3257\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2942 - val_loss: 0.3189\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2954 - val_loss: 0.3169\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2932 - val_loss: 0.3177\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2935 - val_loss: 0.3246\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2918 - val_loss: 0.3172\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2922 - val_loss: 0.3212\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2910 - val_loss: 0.3208\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2923 - val_loss: 0.3188\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2905 - val_loss: 0.3190\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2896 - val_loss: 0.3231\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2910 - val_loss: 0.3198\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2890 - val_loss: 0.3140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2876 - val_loss: 0.3232\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.2887 - val_loss: 0.3188\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2883 - val_loss: 0.3195\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2887 - val_loss: 0.3170\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2871 - val_loss: 0.3204\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2884 - val_loss: 0.3223\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2860 - val_loss: 0.3369\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2884 - val_loss: 0.3192\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2855 - val_loss: 0.3309\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2863 - val_loss: 0.3177\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2863 - val_loss: 0.3316\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2856 - val_loss: 0.3233\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2850 - val_loss: 0.3162\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2853 - val_loss: 0.3229\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2843 - val_loss: 0.3243\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2831 - val_loss: 0.3153\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2845 - val_loss: 0.3211\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2830 - val_loss: 0.3229\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 444us/step - loss: 0.2835 - val_loss: 0.3169\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2836 - val_loss: 0.3129\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.2834 - val_loss: 0.3126\n",
      "121/121 [==============================] - 0s 252us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 630us/step - loss: 1.3816 - val_loss: 0.6719\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.5581 - val_loss: 0.5012\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4476 - val_loss: 0.4397\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.4174 - val_loss: 0.4194\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3996 - val_loss: 0.4084\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3875 - val_loss: 0.3926\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3787 - val_loss: 0.3884\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3690 - val_loss: 0.4107\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3648 - val_loss: 0.3804\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3593 - val_loss: 0.3781\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3556 - val_loss: 0.3805\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3526 - val_loss: 0.3661\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3466 - val_loss: 0.3660\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3467 - val_loss: 0.3616\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3430 - val_loss: 0.3654\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3400 - val_loss: 0.3593\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3375 - val_loss: 0.3607\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3358 - val_loss: 0.3494\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3333 - val_loss: 0.3581\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3289 - val_loss: 0.3523\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3292 - val_loss: 0.3481\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3265 - val_loss: 0.3527\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3268 - val_loss: 0.3466\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3236 - val_loss: 0.3424\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3239 - val_loss: 0.3519\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3220 - val_loss: 0.3385\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3206 - val_loss: 0.3383\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3166 - val_loss: 0.3399\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3159 - val_loss: 0.3341\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3125 - val_loss: 0.3370\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3124 - val_loss: 0.3367\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3116 - val_loss: 0.3276\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3087 - val_loss: 0.3308\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3072 - val_loss: 0.3507\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3074 - val_loss: 0.3261\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3064 - val_loss: 0.3263\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3076 - val_loss: 0.3239\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3032 - val_loss: 0.3315\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3030 - val_loss: 0.3288\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3035 - val_loss: 0.3336\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3013 - val_loss: 0.3442\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2996 - val_loss: 0.3173\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2990 - val_loss: 0.3235\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2973 - val_loss: 0.3258\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2982 - val_loss: 0.3228\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2948 - val_loss: 0.3250\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2960 - val_loss: 0.3196\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2956 - val_loss: 0.3229\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2948 - val_loss: 0.3196\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2941 - val_loss: 0.3134\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2946 - val_loss: 0.3319\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2928 - val_loss: 0.3166\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2941 - val_loss: 0.3212\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2924 - val_loss: 0.3195\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2926 - val_loss: 0.3323\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2908 - val_loss: 0.3187\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2897 - val_loss: 0.3356\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2920 - val_loss: 0.3149\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2883 - val_loss: 0.3167\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2889 - val_loss: 0.3191\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2880 - val_loss: 0.3224\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2882 - val_loss: 0.3207\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2886 - val_loss: 0.3137\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2861 - val_loss: 0.3294\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2866 - val_loss: 0.3231\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2867 - val_loss: 0.3254\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2852 - val_loss: 0.3115\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2854 - val_loss: 0.3338\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2851 - val_loss: 0.3124\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2853 - val_loss: 0.3187\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2852 - val_loss: 0.3159\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2846 - val_loss: 0.3138\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2841 - val_loss: 0.3125\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 446us/step - loss: 0.2838 - val_loss: 0.3186\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.2845 - val_loss: 0.3090\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.2830 - val_loss: 0.3105\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 450us/step - loss: 0.2827 - val_loss: 0.3094\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 454us/step - loss: 0.2835 - val_loss: 0.3268\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.2813 - val_loss: 0.3158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.2813 - val_loss: 0.3240\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.2804 - val_loss: 0.3126\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2810 - val_loss: 0.3102\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2796 - val_loss: 0.3113\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2791 - val_loss: 0.3185\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2792 - val_loss: 0.3140\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2802 - val_loss: 0.3095\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2794 - val_loss: 0.3099\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2792 - val_loss: 0.3187\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2781 - val_loss: 0.3140\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2777 - val_loss: 0.3153\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 442us/step - loss: 0.2776 - val_loss: 0.3104\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2771 - val_loss: 0.3252\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2774 - val_loss: 0.3035\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2749 - val_loss: 0.3179\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2766 - val_loss: 0.3120\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2769 - val_loss: 0.3119\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2769 - val_loss: 0.3219\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2767 - val_loss: 0.3258\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2756 - val_loss: 0.3072\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2764 - val_loss: 0.3167\n",
      "121/121 [==============================] - 0s 246us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.8480 - val_loss: 0.5722\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.5025 - val_loss: 0.4811\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.4413 - val_loss: 0.4330\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.4107 - val_loss: 0.4123\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3948 - val_loss: 0.4031\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3836 - val_loss: 0.4220\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3768 - val_loss: 0.3915\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3688 - val_loss: 0.3901\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3648 - val_loss: 0.3767\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3589 - val_loss: 0.3901\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3559 - val_loss: 0.3767\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.3533 - val_loss: 0.3678\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3484 - val_loss: 0.3604\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3456 - val_loss: 0.3632\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3445 - val_loss: 0.3558\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3401 - val_loss: 0.3535\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3394 - val_loss: 0.3548\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3369 - val_loss: 0.3520\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3346 - val_loss: 0.3507\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3320 - val_loss: 0.3445\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3328 - val_loss: 0.3457\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3290 - val_loss: 0.3458\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3276 - val_loss: 0.3429\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3247 - val_loss: 0.3432\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3237 - val_loss: 0.3433\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3235 - val_loss: 0.3461\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3216 - val_loss: 0.3431\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3204 - val_loss: 0.3461\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3178 - val_loss: 0.3320\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3161 - val_loss: 0.3298\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3133 - val_loss: 0.3501\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3149 - val_loss: 0.3263\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3121 - val_loss: 0.3355\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3122 - val_loss: 0.3351\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3124 - val_loss: 0.3350\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3090 - val_loss: 0.3234\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3081 - val_loss: 0.3301\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3089 - val_loss: 0.3231\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3055 - val_loss: 0.3293\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3048 - val_loss: 0.3189\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3042 - val_loss: 0.3194\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3045 - val_loss: 0.3228\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3026 - val_loss: 0.3206\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3025 - val_loss: 0.3299\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3024 - val_loss: 0.3321\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3002 - val_loss: 0.3160\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3025 - val_loss: 0.3192\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2981 - val_loss: 0.3115\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2988 - val_loss: 0.3192\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.2977 - val_loss: 0.3273\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2974 - val_loss: 0.3215\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2975 - val_loss: 0.3184\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2958 - val_loss: 0.3120\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2946 - val_loss: 0.3268\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2955 - val_loss: 0.3205\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2954 - val_loss: 0.3792\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2979 - val_loss: 0.3591\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2972 - val_loss: 0.3314\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2908 - val_loss: 0.3137\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2906 - val_loss: 0.3094\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2890 - val_loss: 0.3211\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2893 - val_loss: 0.3086\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2880 - val_loss: 0.3105\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2878 - val_loss: 0.3329\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2864 - val_loss: 0.3197\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2858 - val_loss: 0.3337\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2866 - val_loss: 0.3062\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.2839 - val_loss: 0.3038\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2845 - val_loss: 0.3091\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2839 - val_loss: 0.3021\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2841 - val_loss: 0.3092\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2820 - val_loss: 0.3141\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2829 - val_loss: 0.3050\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2834 - val_loss: 0.3088\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2820 - val_loss: 0.3317\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2819 - val_loss: 0.3205\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2782 - val_loss: 0.3066\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.2808 - val_loss: 0.3028\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2805 - val_loss: 0.3120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2809 - val_loss: 0.3030\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2787 - val_loss: 0.3085\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2799 - val_loss: 0.3235\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2790 - val_loss: 0.3234\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2773 - val_loss: 0.3011\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 443us/step - loss: 0.2763 - val_loss: 0.3025\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2783 - val_loss: 0.3000\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2753 - val_loss: 0.3191\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2767 - val_loss: 0.3077\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2770 - val_loss: 0.3077\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2761 - val_loss: 0.2988\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2738 - val_loss: 0.3130\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2750 - val_loss: 0.2982\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2729 - val_loss: 0.2978\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2747 - val_loss: 0.2964\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2744 - val_loss: 0.3037\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2720 - val_loss: 0.3043\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2716 - val_loss: 0.3017\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2727 - val_loss: 0.3028\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2711 - val_loss: 0.2954\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2719 - val_loss: 0.3340\n",
      "121/121 [==============================] - 0s 256us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.8531 - val_loss: 0.5802\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.5036 - val_loss: 0.4955\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4572 - val_loss: 0.4677\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4324 - val_loss: 0.4377\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4121 - val_loss: 0.4187\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3983 - val_loss: 0.4151\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3930 - val_loss: 0.4036\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3835 - val_loss: 0.3926\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3797 - val_loss: 0.3967\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3761 - val_loss: 0.3917\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3705 - val_loss: 0.3842\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3710 - val_loss: 0.3887\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3696 - val_loss: 0.3760\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3667 - val_loss: 0.3752\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3644 - val_loss: 0.3782\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3625 - val_loss: 0.3778\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3606 - val_loss: 0.3749\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3580 - val_loss: 0.3726\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3601 - val_loss: 0.3649\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3569 - val_loss: 0.3782\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3528 - val_loss: 0.3706\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3547 - val_loss: 0.3692\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3518 - val_loss: 0.3765\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.3510 - val_loss: 0.3618\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.3481 - val_loss: 0.3612\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 458us/step - loss: 0.3456 - val_loss: 0.3569\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.3442 - val_loss: 0.3523\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3432 - val_loss: 0.3627\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3420 - val_loss: 0.3558\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3403 - val_loss: 0.3488\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3369 - val_loss: 0.3532\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3381 - val_loss: 0.3470\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3356 - val_loss: 0.4145\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.3341 - val_loss: 0.3427\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3314 - val_loss: 0.3429\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 437us/step - loss: 0.3319 - val_loss: 0.3473\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3318 - val_loss: 0.3407\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3277 - val_loss: 0.3373\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3293 - val_loss: 0.3396\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3303 - val_loss: 0.3393\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 454us/step - loss: 0.3266 - val_loss: 0.3471\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3234 - val_loss: 0.3341\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 440us/step - loss: 0.3244 - val_loss: 0.3433\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3233 - val_loss: 0.3289\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3200 - val_loss: 0.3657\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3218 - val_loss: 0.3331\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.3201 - val_loss: 0.3346\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3165 - val_loss: 0.3277\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3194 - val_loss: 0.3354\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3154 - val_loss: 0.3353\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3141 - val_loss: 0.3374\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3129 - val_loss: 0.3394\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3153 - val_loss: 0.3305\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3128 - val_loss: 0.3241\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3115 - val_loss: 0.3280\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3112 - val_loss: 0.3235\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3097 - val_loss: 0.3366\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3089 - val_loss: 0.3209\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3053 - val_loss: 0.3290\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3043 - val_loss: 0.3198\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3038 - val_loss: 0.3323\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3055 - val_loss: 0.3288\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.3030 - val_loss: 0.3138\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3024 - val_loss: 0.3142\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3017 - val_loss: 0.3141\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3002 - val_loss: 0.3219\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2988 - val_loss: 0.3197\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2984 - val_loss: 0.3136\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2968 - val_loss: 0.3134\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2982 - val_loss: 0.3087\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2978 - val_loss: 0.3113\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2959 - val_loss: 0.3252\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2963 - val_loss: 0.3247\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3001 - val_loss: 0.3114\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2970 - val_loss: 0.3095\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.2966 - val_loss: 0.3259\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2929 - val_loss: 0.3244\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2943 - val_loss: 0.3049\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2951 - val_loss: 0.3098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2928 - val_loss: 0.3072\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2931 - val_loss: 0.3093\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2932 - val_loss: 0.3208\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2920 - val_loss: 0.3061\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2908 - val_loss: 0.3064\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2919 - val_loss: 0.3043\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2903 - val_loss: 0.3100\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2887 - val_loss: 0.3194\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.2891 - val_loss: 0.3060\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2876 - val_loss: 0.3015\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2892 - val_loss: 0.3119\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2860 - val_loss: 0.3048\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2857 - val_loss: 0.3030\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2890 - val_loss: 0.3034\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2854 - val_loss: 0.3070\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2863 - val_loss: 0.3221\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2877 - val_loss: 0.3050\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2840 - val_loss: 0.2987\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2827 - val_loss: 0.3107\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2843 - val_loss: 0.3003\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2833 - val_loss: 0.3030\n",
      "121/121 [==============================] - 0s 240us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.9229 - val_loss: 0.6459\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.5634 - val_loss: 0.5495\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.5080 - val_loss: 0.7413\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.6155 - val_loss: 0.4740\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4374 - val_loss: 0.4504\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4153 - val_loss: 0.4459\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4027 - val_loss: 0.4175\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3937 - val_loss: 0.4152\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3853 - val_loss: 0.4082\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3777 - val_loss: 0.3971\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3720 - val_loss: 0.3941\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3668 - val_loss: 0.3877\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3629 - val_loss: 0.3895\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3574 - val_loss: 0.3832\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3545 - val_loss: 0.3876\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3517 - val_loss: 0.3743\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3494 - val_loss: 0.3725\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3472 - val_loss: 0.3743\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3460 - val_loss: 0.3675\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3425 - val_loss: 0.3720\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3410 - val_loss: 0.3642\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3392 - val_loss: 0.3606\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3377 - val_loss: 0.3655\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3354 - val_loss: 0.3585\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3346 - val_loss: 0.3575\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3330 - val_loss: 0.3574\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3309 - val_loss: 0.3647\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3301 - val_loss: 0.3642\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3287 - val_loss: 0.3537\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3271 - val_loss: 0.3502\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3266 - val_loss: 0.3532\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3240 - val_loss: 0.3504\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3241 - val_loss: 0.3483\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3239 - val_loss: 0.3461\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3210 - val_loss: 0.3457\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3216 - val_loss: 0.3498\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3204 - val_loss: 0.3443\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3176 - val_loss: 0.3474\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3181 - val_loss: 0.3406\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3168 - val_loss: 0.3467\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3159 - val_loss: 0.3387\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3154 - val_loss: 0.3391\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3132 - val_loss: 0.3424\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3117 - val_loss: 0.3347\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3093 - val_loss: 0.3392\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3086 - val_loss: 0.3378\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3083 - val_loss: 0.3270\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3069 - val_loss: 0.3391\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3065 - val_loss: 0.3279\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3038 - val_loss: 0.3282\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3027 - val_loss: 0.3284\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3019 - val_loss: 0.3247\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3004 - val_loss: 0.3227\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3005 - val_loss: 0.3303\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2999 - val_loss: 0.3252\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2984 - val_loss: 0.3190\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2979 - val_loss: 0.3232\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2968 - val_loss: 0.3176\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2956 - val_loss: 0.3240\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2945 - val_loss: 0.3214\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2952 - val_loss: 0.3218\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2931 - val_loss: 0.3140\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2930 - val_loss: 0.3174\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2942 - val_loss: 0.3178\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2925 - val_loss: 0.3168\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2912 - val_loss: 0.3155\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2910 - val_loss: 0.3234\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2896 - val_loss: 0.3135\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2887 - val_loss: 0.3253\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 438us/step - loss: 0.2898 - val_loss: 0.3118\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2888 - val_loss: 0.3193\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2885 - val_loss: 0.3116\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.2876 - val_loss: 0.3152\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.2884 - val_loss: 0.3146\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 446us/step - loss: 0.2886 - val_loss: 0.3227\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.2858 - val_loss: 0.3211\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.2857 - val_loss: 0.3137\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 454us/step - loss: 0.2856 - val_loss: 0.3123\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.2842 - val_loss: 0.3141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 445us/step - loss: 0.2853 - val_loss: 0.3075\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.2832 - val_loss: 0.3099\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2839 - val_loss: 0.3105\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2833 - val_loss: 0.3166\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2846 - val_loss: 0.3136\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.2839 - val_loss: 0.3070\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2822 - val_loss: 0.3040\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2820 - val_loss: 0.3126\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2824 - val_loss: 0.3127\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2821 - val_loss: 0.3177\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2821 - val_loss: 0.3172\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2818 - val_loss: 0.3143\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2813 - val_loss: 0.3119\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2821 - val_loss: 0.3023\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2809 - val_loss: 0.3099\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.2791 - val_loss: 0.3165\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.2811 - val_loss: 0.3052\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.2784 - val_loss: 0.3062\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2780 - val_loss: 0.3062\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2782 - val_loss: 0.3004\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2772 - val_loss: 0.3088\n",
      "121/121 [==============================] - 0s 256us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.9237 - val_loss: 0.6580\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.5794 - val_loss: 0.5288\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4889 - val_loss: 0.4975\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4537 - val_loss: 0.4474\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4264 - val_loss: 0.4267\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.4138 - val_loss: 0.4250\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.4051 - val_loss: 0.4252\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3988 - val_loss: 0.3982\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3888 - val_loss: 0.3995\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3849 - val_loss: 0.3951\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3823 - val_loss: 0.3921\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3776 - val_loss: 0.3879\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3734 - val_loss: 0.3765\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3700 - val_loss: 0.3794\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3672 - val_loss: 0.3770\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3641 - val_loss: 0.3743\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3609 - val_loss: 0.3669\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3577 - val_loss: 0.3677\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3560 - val_loss: 0.3689\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3539 - val_loss: 0.3694\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3540 - val_loss: 0.3614\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3501 - val_loss: 0.3621\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3495 - val_loss: 0.3603\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3473 - val_loss: 0.3645\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3449 - val_loss: 0.3684\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3427 - val_loss: 0.3529\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 429us/step - loss: 0.3422 - val_loss: 0.3556\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3399 - val_loss: 0.3515\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3376 - val_loss: 0.3523\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3388 - val_loss: 0.3652\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3366 - val_loss: 0.3504\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3353 - val_loss: 0.3458\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3347 - val_loss: 0.3442\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3322 - val_loss: 0.3462\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3302 - val_loss: 0.3432\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3296 - val_loss: 0.3476\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3284 - val_loss: 0.3432\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3260 - val_loss: 0.3398\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3251 - val_loss: 0.3474\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3254 - val_loss: 0.3383\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3239 - val_loss: 0.3435\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3228 - val_loss: 0.3389\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3202 - val_loss: 0.3545\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3217 - val_loss: 0.3402\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3201 - val_loss: 0.3439\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3203 - val_loss: 0.3429\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3188 - val_loss: 0.3338\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3177 - val_loss: 0.3388\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3173 - val_loss: 0.3361\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3161 - val_loss: 0.3352\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3127 - val_loss: 0.3421\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3138 - val_loss: 0.3487\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3136 - val_loss: 0.3402\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3118 - val_loss: 0.3299\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3117 - val_loss: 0.3310\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3101 - val_loss: 0.3312\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3087 - val_loss: 0.3257\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3073 - val_loss: 0.3298\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3088 - val_loss: 0.3256\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3063 - val_loss: 0.3256\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.3073 - val_loss: 0.3232\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3053 - val_loss: 0.3286\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3045 - val_loss: 0.3235\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3029 - val_loss: 0.3220\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3033 - val_loss: 0.3246\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3009 - val_loss: 0.3300\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 443us/step - loss: 0.3011 - val_loss: 0.3215\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3006 - val_loss: 0.3230\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3009 - val_loss: 0.3158\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.2986 - val_loss: 0.3252\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3005 - val_loss: 0.3307\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2981 - val_loss: 0.3479\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3020 - val_loss: 0.3299\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2992 - val_loss: 0.3357\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.2987 - val_loss: 0.3450\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2969 - val_loss: 0.3227\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 428us/step - loss: 0.2957 - val_loss: 0.3284\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2940 - val_loss: 0.3092\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.2937 - val_loss: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.2938 - val_loss: 0.3187\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2945 - val_loss: 0.3260\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2921 - val_loss: 0.3232\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2911 - val_loss: 0.3093\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2901 - val_loss: 0.3138\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2909 - val_loss: 0.3275\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2916 - val_loss: 0.4279\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3048 - val_loss: 0.4283\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3042 - val_loss: 0.3328\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2911 - val_loss: 0.3035\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2888 - val_loss: 0.3140\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2881 - val_loss: 0.3027\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2874 - val_loss: 0.3039\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.2869 - val_loss: 0.3046\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.2853 - val_loss: 0.3012\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.2867 - val_loss: 0.2980\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2860 - val_loss: 0.2981\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.2856 - val_loss: 0.3166\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.2841 - val_loss: 0.3134\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.2846 - val_loss: 0.3146\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.2835 - val_loss: 0.3169\n",
      "121/121 [==============================] - 0s 236us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.9107 - val_loss: 2.2374\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.7423 - val_loss: 0.5452\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.4994 - val_loss: 0.5061\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4747 - val_loss: 0.4884\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.4557 - val_loss: 0.4708\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.4486 - val_loss: 0.4638\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4398 - val_loss: 0.4573\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4342 - val_loss: 0.4483\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4302 - val_loss: 0.4455\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4266 - val_loss: 0.4428\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4221 - val_loss: 0.4339\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4206 - val_loss: 0.4405\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4185 - val_loss: 0.4325\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4150 - val_loss: 0.4331\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.4159 - val_loss: 0.4272\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4120 - val_loss: 0.4230\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.4102 - val_loss: 0.4235\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.4060 - val_loss: 0.4239\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4036 - val_loss: 0.4331\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.4040 - val_loss: 0.4191\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4030 - val_loss: 0.4166\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.4007 - val_loss: 0.4100\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3984 - val_loss: 0.4116\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3966 - val_loss: 0.4171\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3972 - val_loss: 0.4093\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 439us/step - loss: 0.3952 - val_loss: 0.4087\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 436us/step - loss: 0.3934 - val_loss: 0.4068\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3912 - val_loss: 0.4284\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3900 - val_loss: 0.4058\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3893 - val_loss: 0.4014\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3881 - val_loss: 0.4038\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3858 - val_loss: 0.3973\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3862 - val_loss: 0.4033\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3865 - val_loss: 0.3944\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 403us/step - loss: 0.3860 - val_loss: 0.4013\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 403us/step - loss: 0.3839 - val_loss: 0.3980\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3831 - val_loss: 0.3945\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3827 - val_loss: 0.3913\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3810 - val_loss: 0.3939\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3792 - val_loss: 0.3904\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3808 - val_loss: 0.3920\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3791 - val_loss: 0.3963\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3798 - val_loss: 0.3937\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3796 - val_loss: 0.3916\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3789 - val_loss: 0.3842\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3788 - val_loss: 0.3847\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3782 - val_loss: 0.3842\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3764 - val_loss: 0.3858\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3762 - val_loss: 0.3820\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3752 - val_loss: 0.3864\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3761 - val_loss: 0.3835\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3767 - val_loss: 0.3893\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3754 - val_loss: 0.3858\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3751 - val_loss: 0.3804\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3750 - val_loss: 0.3804\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3741 - val_loss: 0.3828\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3731 - val_loss: 0.3805\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3717 - val_loss: 0.3814\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3723 - val_loss: 0.3843\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3704 - val_loss: 0.3904\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3721 - val_loss: 0.3866\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3725 - val_loss: 0.3774\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3691 - val_loss: 0.3829\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3703 - val_loss: 0.3781\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3698 - val_loss: 0.3749\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3703 - val_loss: 0.3841\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3702 - val_loss: 0.3758\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3712 - val_loss: 0.3775\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3679 - val_loss: 0.3753\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3683 - val_loss: 0.3813\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3673 - val_loss: 0.3792\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3683 - val_loss: 0.3829\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3675 - val_loss: 0.3737\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3683 - val_loss: 0.3770\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3658 - val_loss: 0.3776\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3659 - val_loss: 0.3800\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3669 - val_loss: 0.3789\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3671 - val_loss: 0.3760\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3656 - val_loss: 0.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3650 - val_loss: 0.3756\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3649 - val_loss: 0.3745\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3683 - val_loss: 0.3763\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3660 - val_loss: 0.3737\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3650 - val_loss: 0.3738\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3623 - val_loss: 0.3748\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3638 - val_loss: 0.3779\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3633 - val_loss: 0.3755\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3641 - val_loss: 0.3766\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3617 - val_loss: 0.3995\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3619 - val_loss: 0.3712\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3633 - val_loss: 0.3724\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3612 - val_loss: 0.3749\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3610 - val_loss: 0.3721\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3616 - val_loss: 0.3716\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3614 - val_loss: 0.3724\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3610 - val_loss: 0.3808\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3604 - val_loss: 0.3703\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3593 - val_loss: 0.3714\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3586 - val_loss: 0.3775\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3615 - val_loss: 0.3771\n",
      "121/121 [==============================] - 0s 231us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 1.4730 - val_loss: 3.6646\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 1.8413 - val_loss: 0.5482\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.5029 - val_loss: 0.5040\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4532 - val_loss: 0.4548\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.4249 - val_loss: 0.4400\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 450us/step - loss: 0.4103 - val_loss: 0.4294\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3991 - val_loss: 0.4151\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3919 - val_loss: 0.4125\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3858 - val_loss: 0.4086\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3824 - val_loss: 0.4007\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 452us/step - loss: 0.3793 - val_loss: 0.4028\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.3770 - val_loss: 0.3962\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3749 - val_loss: 0.3932\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3731 - val_loss: 0.3926\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3714 - val_loss: 0.3897\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3697 - val_loss: 0.3892\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3675 - val_loss: 0.3910\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3661 - val_loss: 0.3903\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3653 - val_loss: 0.3858\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3644 - val_loss: 0.3854\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3623 - val_loss: 0.3863\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3623 - val_loss: 0.3861\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3627 - val_loss: 0.3856\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.3602 - val_loss: 0.3823\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3606 - val_loss: 0.3811\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3601 - val_loss: 0.3826\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3596 - val_loss: 0.3826\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3597 - val_loss: 0.3822\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3583 - val_loss: 0.3878\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3591 - val_loss: 0.3883\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3578 - val_loss: 0.3806\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3577 - val_loss: 0.3778\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 430us/step - loss: 0.3573 - val_loss: 0.3856\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3577 - val_loss: 0.3839\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3577 - val_loss: 0.3827\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3554 - val_loss: 0.3777\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3565 - val_loss: 0.3801\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3559 - val_loss: 0.3845\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3558 - val_loss: 0.3774\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3551 - val_loss: 0.3768\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3538 - val_loss: 0.3745\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3546 - val_loss: 0.3815\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3544 - val_loss: 0.3753\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3528 - val_loss: 0.3770\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3527 - val_loss: 0.3780\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3542 - val_loss: 0.3767\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3515 - val_loss: 0.3792\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3531 - val_loss: 0.3769\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3513 - val_loss: 0.3787\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3512 - val_loss: 0.3845\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3514 - val_loss: 0.3727\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3512 - val_loss: 0.3777\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3495 - val_loss: 0.3743\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3492 - val_loss: 0.3711\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3482 - val_loss: 0.3702\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3488 - val_loss: 0.3824\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3484 - val_loss: 0.3797\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 435us/step - loss: 0.3474 - val_loss: 0.3685\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3474 - val_loss: 0.3709\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3475 - val_loss: 0.3732\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3475 - val_loss: 0.3715\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3465 - val_loss: 0.3692\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3466 - val_loss: 0.3735\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.3467 - val_loss: 0.3687\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3465 - val_loss: 0.3710\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3468 - val_loss: 0.3679\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3452 - val_loss: 0.3731\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3454 - val_loss: 0.3687\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3453 - val_loss: 0.3653\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3454 - val_loss: 0.3658\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3456 - val_loss: 0.3676\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3458 - val_loss: 0.3688\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3452 - val_loss: 0.3680\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3449 - val_loss: 0.3718\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3455 - val_loss: 0.3651\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.3454 - val_loss: 0.3697\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3459 - val_loss: 0.3687\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 410us/step - loss: 0.3442 - val_loss: 0.3674\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3437 - val_loss: 0.3649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3432 - val_loss: 0.3652\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 458us/step - loss: 0.3435 - val_loss: 0.3690\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 441us/step - loss: 0.3434 - val_loss: 0.3642\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 447us/step - loss: 0.3436 - val_loss: 0.3653\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3423 - val_loss: 0.3641\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3426 - val_loss: 0.3620\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3421 - val_loss: 0.3649\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3417 - val_loss: 0.3642\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3424 - val_loss: 0.3664\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3422 - val_loss: 0.3624\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3415 - val_loss: 0.3607\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3415 - val_loss: 0.3651\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3423 - val_loss: 0.3653\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3416 - val_loss: 0.3770\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3417 - val_loss: 0.3648\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3404 - val_loss: 0.3711\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3411 - val_loss: 0.3686\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3411 - val_loss: 0.3645\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3411 - val_loss: 0.3643\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3398 - val_loss: 0.3629\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3390 - val_loss: 0.3625\n",
      "121/121 [==============================] - 0s 226us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 1.3128 - val_loss: 0.7203\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.6379 - val_loss: 0.5998\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 406us/step - loss: 0.5565 - val_loss: 0.5428\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.5142 - val_loss: 0.5126\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4883 - val_loss: 0.4953\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 411us/step - loss: 0.4688 - val_loss: 0.4833\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.4523 - val_loss: 0.4571\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 405us/step - loss: 0.4409 - val_loss: 0.4488\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.4337 - val_loss: 0.4404\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4254 - val_loss: 0.4348\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 409us/step - loss: 0.4202 - val_loss: 0.4325\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.4141 - val_loss: 0.4248\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.4089 - val_loss: 0.4194\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4059 - val_loss: 0.4151\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.4034 - val_loss: 0.4192\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.4017 - val_loss: 0.4123\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3996 - val_loss: 0.4187\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 426us/step - loss: 0.3973 - val_loss: 0.4084\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 434us/step - loss: 0.3944 - val_loss: 0.4047\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3935 - val_loss: 0.4055\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3919 - val_loss: 0.4034\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3916 - val_loss: 0.4046\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3904 - val_loss: 0.4006\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3877 - val_loss: 0.4023\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3864 - val_loss: 0.3953\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3852 - val_loss: 0.3975\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3839 - val_loss: 0.3916\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3839 - val_loss: 0.3969\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3857 - val_loss: 0.3942\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3834 - val_loss: 0.3944\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3803 - val_loss: 0.3925\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3802 - val_loss: 0.3908\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3798 - val_loss: 0.3891\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3784 - val_loss: 0.3916\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3780 - val_loss: 0.3918\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3800 - val_loss: 0.3924\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3778 - val_loss: 0.3896\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3755 - val_loss: 0.3869\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3758 - val_loss: 0.3918\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3761 - val_loss: 0.3862\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3770 - val_loss: 0.3838\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3755 - val_loss: 0.3831\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3732 - val_loss: 0.3866\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 427us/step - loss: 0.3733 - val_loss: 0.3825\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3737 - val_loss: 0.4070\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 425us/step - loss: 0.3748 - val_loss: 0.3782\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3703 - val_loss: 0.3884\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3715 - val_loss: 0.3790\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3713 - val_loss: 0.3831\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3709 - val_loss: 0.3912\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3694 - val_loss: 0.3896\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3710 - val_loss: 0.3792\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3737 - val_loss: 0.3813\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 412us/step - loss: 0.3704 - val_loss: 0.3766\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3684 - val_loss: 0.3878\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3699 - val_loss: 0.3797\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3681 - val_loss: 0.3829\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3681 - val_loss: 0.3877\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3686 - val_loss: 0.3805\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3671 - val_loss: 0.3806\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3668 - val_loss: 0.3799\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3659 - val_loss: 0.3784\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3664 - val_loss: 0.3846\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3659 - val_loss: 0.3776\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3656 - val_loss: 0.3763\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3654 - val_loss: 0.3796\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3654 - val_loss: 0.3793\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3654 - val_loss: 0.3785\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3660 - val_loss: 0.3831\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3646 - val_loss: 0.3849\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 424us/step - loss: 0.3653 - val_loss: 0.3735\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3642 - val_loss: 0.3773\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 432us/step - loss: 0.3657 - val_loss: 0.3752\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 433us/step - loss: 0.3635 - val_loss: 0.3753\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 431us/step - loss: 0.3637 - val_loss: 0.3748\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3636 - val_loss: 0.3769\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3642 - val_loss: 0.3750\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 422us/step - loss: 0.3637 - val_loss: 0.3741\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3642 - val_loss: 0.3788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3640 - val_loss: 0.3889\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 417us/step - loss: 0.3618 - val_loss: 0.3760\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3618 - val_loss: 0.3782\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3623 - val_loss: 0.3735\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 423us/step - loss: 0.3619 - val_loss: 0.3720\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3594 - val_loss: 0.3754\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3605 - val_loss: 0.3835\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3608 - val_loss: 0.3750\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3605 - val_loss: 0.3739\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3603 - val_loss: 0.3802\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3611 - val_loss: 0.3773\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3618 - val_loss: 0.3715\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 415us/step - loss: 0.3611 - val_loss: 0.3727\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3577 - val_loss: 0.3701\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 419us/step - loss: 0.3597 - val_loss: 0.3746\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 413us/step - loss: 0.3588 - val_loss: 0.3791\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 421us/step - loss: 0.3601 - val_loss: 0.3737\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 420us/step - loss: 0.3566 - val_loss: 0.3980\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 414us/step - loss: 0.3577 - val_loss: 0.3738\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 416us/step - loss: 0.3586 - val_loss: 0.3772\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 418us/step - loss: 0.3605 - val_loss: 0.3681\n",
      "121/121 [==============================] - 0s 246us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharymessinger/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.76499079 0.76915904 0.75124055 0.75197138        nan 0.75745161\n",
      " 0.74226994 0.76035706 0.76840029 0.72920487]\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.7610 - val_loss: 0.5434\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.4696 - val_loss: 0.5366\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.4285 - val_loss: 0.4274\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.4047 - val_loss: 0.4368\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3980 - val_loss: 0.4030\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3927 - val_loss: 0.3948\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3745 - val_loss: 0.3865\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.3676 - val_loss: 0.3833\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3609 - val_loss: 0.3704\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3550 - val_loss: 0.3663\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.3523 - val_loss: 0.3625\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 443us/step - loss: 0.3497 - val_loss: 0.3573\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.3453 - val_loss: 0.3630\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3436 - val_loss: 0.3554\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3421 - val_loss: 0.3702\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.3392 - val_loss: 0.3583\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.3376 - val_loss: 0.3488\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3351 - val_loss: 0.3432\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3322 - val_loss: 0.3472\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 388us/step - loss: 0.3307 - val_loss: 0.3493\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3288 - val_loss: 0.3692\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 354us/step - loss: 0.3262 - val_loss: 0.3477\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 357us/step - loss: 0.3244 - val_loss: 0.3336\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 372us/step - loss: 0.3230 - val_loss: 0.3387\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3215 - val_loss: 0.3344\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.3214 - val_loss: 0.3420\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 375us/step - loss: 0.3191 - val_loss: 0.3319\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.3169 - val_loss: 0.3335\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.3147 - val_loss: 0.3570\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 386us/step - loss: 0.3135 - val_loss: 0.3401\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.3133 - val_loss: 0.3200\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3105 - val_loss: 0.3330\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3097 - val_loss: 0.3198\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3090 - val_loss: 0.3193\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.3069 - val_loss: 0.3216\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.3040 - val_loss: 0.3162\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3053 - val_loss: 0.3690\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3072 - val_loss: 0.3240\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 426us/step - loss: 0.3017 - val_loss: 0.3153\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2996 - val_loss: 0.3107\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.2999 - val_loss: 0.3133\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2984 - val_loss: 0.3193\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.2977 - val_loss: 0.3172\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.3003 - val_loss: 0.3061\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2968 - val_loss: 0.3261\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2946 - val_loss: 0.3081\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.2917 - val_loss: 0.3082\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.2915 - val_loss: 0.3038\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.2907 - val_loss: 0.3070\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2901 - val_loss: 0.3002\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.2909 - val_loss: 0.3213\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2927 - val_loss: 0.3018\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2878 - val_loss: 0.3218\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.2873 - val_loss: 0.3117\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2860 - val_loss: 0.2948\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.2854 - val_loss: 0.3293\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.2856 - val_loss: 0.3050\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.2835 - val_loss: 0.3354\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.2819 - val_loss: 0.3006\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.2816 - val_loss: 0.2927\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2806 - val_loss: 0.3077\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 376us/step - loss: 0.2806 - val_loss: 0.2975\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 373us/step - loss: 0.2788 - val_loss: 0.3054\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2788 - val_loss: 0.2975\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2769 - val_loss: 0.2957\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2799 - val_loss: 0.3067\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2774 - val_loss: 0.2925\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2770 - val_loss: 0.3036\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.2801 - val_loss: 0.2980\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2854 - val_loss: 0.3035\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2775 - val_loss: 0.2952\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2791 - val_loss: 0.3064\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2772 - val_loss: 0.2895\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 386us/step - loss: 0.2763 - val_loss: 0.2986\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 386us/step - loss: 0.2747 - val_loss: 0.3052\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2759 - val_loss: 0.2978\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2754 - val_loss: 0.2930\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.2746 - val_loss: 0.2946\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 390us/step - loss: 0.2741 - val_loss: 0.2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2726 - val_loss: 0.3000\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 390us/step - loss: 0.2754 - val_loss: 0.2914\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.2747 - val_loss: 0.2891\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.2716 - val_loss: 0.2997\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 385us/step - loss: 0.2727 - val_loss: 0.2931\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 380us/step - loss: 0.2719 - val_loss: 0.2844\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2725 - val_loss: 0.2905\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.2699 - val_loss: 0.2884\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 377us/step - loss: 0.2704 - val_loss: 0.2904\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2704 - val_loss: 0.3044\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 382us/step - loss: 0.2710 - val_loss: 0.2821\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 378us/step - loss: 0.2703 - val_loss: 0.2837\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.2697 - val_loss: 0.2959\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2701 - val_loss: 0.2932\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 383us/step - loss: 0.2711 - val_loss: 0.2965\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 384us/step - loss: 0.2745 - val_loss: 0.2843\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 379us/step - loss: 0.2751 - val_loss: 0.2931\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 381us/step - loss: 0.2685 - val_loss: 0.2833\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 387us/step - loss: 0.2683 - val_loss: 0.2913\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 388us/step - loss: 0.2677 - val_loss: 0.2862\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 385us/step - loss: 0.2680 - val_loss: 0.3002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(input_shape=[8], learning_rate=0.003, model=&lt;function build_model at 0x2c1dcde10&gt;, n_hidden=1, n_neurons=30),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0003],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [10, 15, 20]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(input_shape=[8], learning_rate=0.003, model=&lt;function build_model at 0x2c1dcde10&gt;, n_hidden=1, n_neurons=30),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0003],\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: [10, 15, 20]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function build_model at 0x2c1dcde10&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tn_hidden=1\n",
       "\tn_neurons=30\n",
       "\tlearning_rate=0.003\n",
       "\tinput_shape=[8]\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function build_model at 0x2c1dcde10&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tn_hidden=1\n",
       "\tn_neurons=30\n",
       "\tlearning_rate=0.003\n",
       "\tinput_shape=[8]\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(input_shape=[8], learning_rate=0.003, model=<function build_model at 0x2c1dcde10>, n_hidden=1, n_neurons=30),\n",
       "                   param_distributions={'learning_rate': [0.0003],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [10, 15, 20]})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\": [10,15,20],\n",
    "    \"learning_rate\": [3e-4]\n",
    "}\n",
    "\n",
    "keras_reg = KerasRegressor(model=build_model, n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8])\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8634b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neurons': 15, 'n_hidden': 3, 'learning_rate': 0.0003}\n",
      "Best Score: 0.7691590408741836\n",
      "Best Model: KerasRegressor(\n",
      "\tmodel=<function build_model at 0x2c1dcde10>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=None\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=1\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tn_hidden=3\n",
      "\tn_neurons=15\n",
      "\tlearning_rate=0.0003\n",
      "\tinput_shape=[8]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Params: {rnd_search_cv.best_params_}\")\n",
    "print(f\"Best Score: {rnd_search_cv.best_score_}\")\n",
    "print(f\"Best Model: {rnd_search_cv.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de89e8",
   "metadata": {},
   "source": [
    "While this implementation works, for more complex problems with larger datasets the training might become too slow and thus can only be used to explore a small portion of the hyperparameter space.\n",
    "\n",
    "Fortunately, there are many techniques to explore a search space much more efficiently than randomly. Here are a few popular Python libraries to explore:\n",
    "\n",
    "* **Hyperopt**\n",
    "* **Hyperas, kopt, or Talos**\n",
    "* **Keras Tuner**\n",
    "* **Scikit-Optimize (skopt)**\n",
    "* **Spearmint**\n",
    "* **Hyperbrand**\n",
    "* **Sklearn-Deap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7cdae9",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc2087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
