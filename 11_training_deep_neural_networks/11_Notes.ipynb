{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe7f922",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d5422",
   "metadata": {},
   "source": [
    "In chapter 10, we introduced ANN's and trained our first deep neural networks, but they were shallow in nature with only a few hidden layers. What if we tackle a much more complex problem, such as detecting hundreds of different types of objects in high-resolution images? You may need to train a much deeper neural network - perhaps with 10 or more hidden layers. But training networks that deep present their own challenges:\n",
    "\n",
    "* You may be faced with the _vanishing grandients_ problem or the related _exploding gradients problem_, which is when the gradients during backpropogation become smaller and smaller or larger and larger, making the lower layers very hard to train\n",
    "* You might not have enough training data for such a large network, or it might be too costly to label.\n",
    "* Training might be too slow.\n",
    "* A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they are too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6187234",
   "metadata": {},
   "source": [
    "## The Vanishing / Exploding Gradients Problem\n",
    "\n",
    "As discussed in the previous chapter, backpropogation works by going from the ouput layer to the input layer, propogating the the error gradients along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortuantely, gradients often get smaller and smaller as the algorithm progress down to the lower layers, and as a result, the GD update leaves the lower layers weights virtually unchanged, which is why it's called the `Vanishing Gradients Problem`.\n",
    "\n",
    "In some cases, the opposite can happen - the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges, which is called the `Exploding Gradients Problem`. The `Exploding Gradients Problem` is more common in recurrent neural networks.\n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients - different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca86a7b",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization\n",
    "\n",
    "`Glorot initialization` (also known as Xavier initialization) and `He initialization` are techniques for initializing the weights of a neural network, and are designed to address the `The Vanishing / Exploding Gradients Problem` of training deep neural networks by providing a better starting point for the optimization process.\n",
    "\n",
    "`Glorot initialization` is recommended for tanh- and sigmoid-based activations functions (e.g None, tanh, logistic, softmax) and `He initialization` is recommended for ReLU-based activation functions.\n",
    "\n",
    "By default keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change to He initialization by setting `kernal_initializer=\"he_uniform\"` or `kernal_initializer=\"he_normal\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5962d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.13.0\n",
      "keras: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "print(f\"Tensorflow: {tf.__version__}\")\n",
    "print(f\"keras: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074fee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x162d26980>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65701a57",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    "\n",
    "One of the key insights in the 2010 paper by Glorot and Benigio was that the problems with unstable graidents were in part due to a poor choice of activation functions. But it turns out that the ReLU activation function performs much better in DNN's, mostly because it does not saturate for positive values and because it is fast to compute.\n",
    "\n",
    "Howeverm the ReLU activation function is not perfect - it suffers from a problem known as the dying ReLU's: during training, some neurons effectively \"die\", meaning they stop outputting anything other than 0. A neuron dies when it's weights get tweaked in such a way that the weighted sum of it's inputs are negative for all instances in the training set. Once a ReLU neuron is in this state, it stops learning and does not contribute to the gradients during backpropagation.\n",
    "\n",
    "To solve this problem, you may want to use a a variants of the ReLU function.\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "The Leaky ReLU introduces a small, non-zero slope for the negative values of the input, which is controlled by the hyperparameter `alpha`, which is typically set `0.01`. How large or small `alpha` is controlls how _leaky_ the ReLU functions becomes.\n",
    "\n",
    "To use the Leaky ReLU in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96376c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 18:43:58.704269: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-29 18:43:58.704312: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-29 18:43:58.704322: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-29 18:43:58.704506: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-29 18:43:58.704835: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4926c70",
   "metadata": {},
   "source": [
    "### Exponential Linear Unit (ELU)\n",
    "\n",
    "The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
    "\n",
    "* It takes on negative values when `z < 0`, which allows the unit to have an average output closer to 0 and helps allevaite the vanishing graidents problem. The `alpha` hyperparameter defines the value that the ELU function approcahes `z` is a large negative number, and it is usually set to 1.\n",
    "* It has a non-zero gradient for `z < 0`, which helps it avoid the dead neurons problem.\n",
    "* If `alpha` is equal to 1 then the function is smooth everywhere, includingaround `z = 0`, which helps speed up GD since it does not bounce as much to the left and right of `z = 0`.\n",
    "\n",
    "The main drawback of the ELU activcation function is that it is slower to compute than the ReLU function and it's variants (due to it's use of exponential function), even though it's faster convergence rate during training will compensate for that slow computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2c32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.ELU(alpha=0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f0e95",
   "metadata": {},
   "source": [
    "### Scaled Exponential Linear Unit\n",
    "\n",
    "Scaled Exponential Linear Unit (SELU) is a variation of the Exponential Linear Unit (ELU) activation function that is it is designed to induce self-normalizing properties in a neural network. The SELU activation function often signficantly outperforms other activation functions for DNN's, however, it needs to meet a few conditions in order to work properly:\n",
    "\n",
    "* The input features must be standardized (mean of 0 and a standard deviation of 1)\n",
    "* Every hidden layers weights must be initialized with LeCun normal initialization. In Keras, this means setting `kernal_initializer=\"lecun_normal\"`.\n",
    "* The networks architecture must be Sequential.\n",
    "* Every hidden layer must be Dense, although there is some research that shows SeLU can improve performance in CNN's as well.\n",
    "\n",
    "To use SeLU in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcdb8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x16298d510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec5f39",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03512e",
   "metadata": {},
   "source": [
    "Although using He initialization calong with ELU (or any varianet of ReLU) can significantly reduce the danger of the vanishing / exploding gradients problems at the beggining of training, it doesn't guarantee that they won't come back during training.\n",
    "\n",
    "`Batching Normalization` is a technique designed to address this problem by adding an operation in the model just before or after the activatino function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new paremeter vectors per layer: one for scaling, and the other for shifting. In other words, the oepration lets the model learn the optimal sacle and mean of each of the layers inputs. In many cases if you ad a BN layer as the first layer of your network, you do not need to standardize your training set!\n",
    "\n",
    "The Batch Normalization Algorithm works by: \n",
    "1.  Calculate the mean of each input across the entire mini-batch.\n",
    "2. Calculate the standard deviation of each input across the entire mini-batch.\n",
    "3. Standardize each input instance in the mini-batch using the mean and standard deviation calculated above.\n",
    "4. Offset and scale each instance after standardization using a different offset parameter and scaling parameter for each instance.\n",
    "\n",
    "Each input will have its own mean, standard deviation, offset and scaling parameters & the offset and scaling parameter vectors are learned through backpropagation. The mean and standard deviation parameter vectors are calculated per mini-batch during training time and not learned during backpropagation.Additionally, batch normalization has regularization effects, reducing the need for other regularization techniques like dropout in some cases. \n",
    "\n",
    "However, there is a runtime penalty: Batch Normalization opten slows down the network when making predictions since there are more computations to perform. However, it is possible to fuse a Batch Normalization layer with previous layer after training to avoid this, through updating the previous layer's weights and biases so that its output matches the scale and offset of the Batch Normalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523ad9c",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0402528",
   "metadata": {},
   "source": [
    "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a `BatchNormalization` layer before or after each hidden layers activation function, and optionally add a BN layer as well as the first layer in your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875703b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 784)               3136      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 300)               1200      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30488c52",
   "metadata": {},
   "source": [
    "Let's look a the parameters of the first BN layer. Two are trainianble (via backpropogtion) and two are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668dc3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_10/gamma:0', True),\n",
       " ('batch_normalization_10/beta:0', True),\n",
       " ('batch_normalization_10/moving_mean:0', False),\n",
       " ('batch_normalization_10/moving_variance:0', False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceac4af",
   "metadata": {},
   "source": [
    "The authors of BN argued in favor adding the BN layers before the activation functions, rather than after (as we just did), but which is preferable seems to depend on the task - you can experiment with this too to se which option works best on your dataset. To add the BN layers before the actviation functoins, you must remove the activation function from the hidden layers and add them as seperate layers after the BN layers.\n",
    "\n",
    "Moreover, since a Batch Normalizatoin layer includes one offset parameter per input, you can remove the bias term fro mthe previous layer (just pass `use_bias=False` when creating it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb842d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 784)               3136      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 300)               1200      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               30000     \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270946 (1.03 MB)\n",
      "Trainable params: 268578 (1.02 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e5099",
   "metadata": {},
   "source": [
    "The `BatchNormalization` class has quite a few hyperparameters we can tweak, although the defaults will usually be fine:\n",
    "\n",
    "* `momentum` - used by the BN layer when it updates to exponential moving averages. A good momentum value is typically close to 1: for example `0.9`, `0.99`, or `0.999`. You may want more 9's for larger datasets and smaller mini batches.\n",
    "* `axis` - determines which axis should be normalized, and defaults to -1 meaning it will normalize the last axis.\n",
    "\n",
    "`BatchNormalization` has become one of hte most-used layers in deep neural networks, to the point that it is often omitted in diagrams as it assumed that BN will be added after every layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84804653",
   "metadata": {},
   "source": [
    "## Gradinet Clippings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96984e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
